\section{Related Work}
\label{sec:related}

Early attempts at simulating grounded language learning focus on
interactions between adults and young children while playing with a
set of objects from different categories \cite{roy1999learning,roy2002learning,
  gorniak2003visually,mukherjee2003visual}. In a representative study
from this series, \citet{roypentland2002learning} use speech recorded from
such interactions paired with different views of the visible objects
to identify linguistic units (i.e.\ words) and visual categories, and
to map these two modalities together. A hard-coded visual system
extracts object representations from images, and spoken utterances are
represented as phoneme probabilities generated by an RNN pre-trained on
spectrograms.  Their experiments on small-scale data (around 20 words
and seven visual categories) show that the model can segment words and
map them to visual categories.

\subsection{Spoken Language Grounded in Images}
\label{sec:images}
The availability of datasets of images associated with spoken captions
such as Flickr Audio Captions \cite{harwath2015deep}, Places
\cite{zhou2014learning} and Spoken COCO \cite{hsu2019transfer} led to
a rapid development of neural models of grounded language learning; see
\citet{chrupala-visually-2021} for a comprehensive overview. In contrast to 
earlier approaches, these models are trained end-to-end directly on
large datasets.

Following the architecture proposed in \citet{karpathy2014deep} the visual and 
speech modality are usually encoded using separate pathways, and subsequently 
mapped into a joint representation space.
Visual features are extracted from a pre-trained
image classification model that processes the whole or a specific
region of an image (however see \citet{harwath2018jointly}, who train the
model end-to-end on images and their spoken captions on the Places
dataset). The audio encoder component in most models is 
either an adaptation of \citet{harwath2016unsupervised} which feeds a
spectrogram of the speech signal to a convolutional architecture, or a
hybrid architecture of convolutional followed by recurrent layers using
Mel-Frequency Cepstral Coefficient (MFCC) features from the audio
signal as input as introduced by \citet{chrupala-etal-2017-representations}.

The majority of models of speech grounded in images are optimized for and evaluated on
image retrieval via spoken caption and vice versa. Additionally, a range of
diagnostic analyses have been performed on the hidden
representations of these models to study whether they encode 
the identity and boundaries of subword units such as phonemes
and syllables \cite{alishahi-etal-2017-encoding, harwath2019towards,
  khorrami_2021} as well as individual words
\cite{chrupala-etal-2017-representations,havard2019word}. Moreover, in
addition to examining form-meaning associations at the utterance
level, \citet{harwath-glass-2017-learning} explicitly learn a lexicon by
extracting audio and image segments, clustering each modality
separately, and mapping them together by calculating the pairwise
similarities of their members in the joint semantic space.

\subsection{Spoken Language Grounded in Video}
\label{sec:video}
There have also been recent attempts to learn spoken language grounded
in video instead of static images.  \citet{boggust2019grounding}
sample audio-visual fragments from cooking videos;  their
grounded model treats video frames as still images and discards their
temporal order.
\citet{rouditchenko2020avlnet} integrate the temporal information when
encoding videos from the Howto100m dataset \cite{miech2019howto100m},
and perform better than previous work in language and video clip
retrieval.

Models trained on instructional video datasets often do not
generalize well to other domains. \citet{monfort2021spokenmoments}
highlight this limitation and show that training on their larger and
more diverse Spoken Moments in Time dataset leads to better
generalization.  But the point remains that these video datasets contain
descriptive speech, thus ensuring that there is a strong correlation
between the spoken language and their visual context, a characteristic
that is not representative of the experience of learning language in
 real world. We remedy this limitation by using a video dataset that does 
 not guarantee a direct description of the visual context. 

\subsection{Child Language Learning from Video}
There are many studies on young children learning language by watching
videos; see \citet{vanderplank2010deja} for a survey. The main takeaway
of these studies is that language learning is much more effective in a
social, conversational setting than by passively watching videos
\cite{kuhl2003foreign,anderson2005television,robb2009just},
but learning does happen in such
contexts. Importantly for our goal, techniques such as the intermodal
preferential looking paradigm have been developed to systematically test young 
language learners' knowledge of words, syntactic structure and semantic roles
\cite{hirsh1996intermodal,bergelson20126,noble2011comprehension}.
\citet{nikolaus-fourtassi-2021-evaluating}
employ this evaluation strategy to test semantic knowledge at word and
sentence level in their computational model of word learning from
images. We adapt this approach to evaluate how our grounded model
associates semantic information to spoken words and utterances from
video.

\subsection{Intra-linguistic Statistics}
One further aspect of learning spoken language via visual grounding is
the fact that grounding is only part of the story. Human children
arguably infer substantial amounts of information about language
structure and meaning from purely intra-linguistic co-occurrence
statistics \citep[e.g.,][]{saffran1996statistical}. A similar mechanism is what 
allows 
written language models
such as BERT \citep{devlin-etal-2019-bert} or GPT-3 \citep{brown2020language} 
to capture and exhibit relatively sophisticated
linguistic knowledge. Loosely similar approaches have started to also
make an impact for the spoken modality
\citep[e.g.][]{wav2vec2,hsu2021hubert}. Here we take a simple
pre-training-based approach to integrating this type of
self-supervision with learning-via-grounding.


