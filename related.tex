\section{Related work}
\label{sec:related}

\paragraph{Spoken language grounded in images.}
Early attempts at simulating grounded language learning focus on interactions between adults and young children while playing with a set of objects from different categories \cite{roy1999learning, roy2000grounded, roy2000learning, roy2002learning, gorniak2003visually, mukherjee2003visual}. In a representative study from these series, \citet{roy2002learning} use speech recorded from such interactions paired with different views of the visible objects to identify linguistic units (i.e.\ words) and visual categories, and to map these two modalities together. A built-in visual system extracts object representations from images, and spoken utterances are represented as phoneme probabilities generated by a pre-trained RNN. Their experiments show that the model can segment words and map them to visual categories in small scale (around 20 words and seven visual categories).

The availability of image-caption datasets such as MS-COCO \cite{lin2014microsoft} and Flickr30K \cite{plummer2015flickr30k} and their spoken counterparts such as Places \cite{zhou2014learning} and Speech COCO \cite{speech_coco} led to a rapid development of deep models of grounded language learning; see \citet{chrupala-visually-2021} for a comprehensive overview. Most of these models adapt the architecture of \citet{karpathy2014deep} by implementing separate pathways for encoding the visual and linguistic modality, and mapping these encodings into a joint representation space. Many existing models extract visual features from a pre-trained image classification model that processes the whole or a specific region of an image (although \citet{harwath2018jointly} train the model end-to-end on images and their spoken captions from the Places dataset). The linguistic encodings are captured by a customized text or speech model.  The audio encoder component in most models is often either an adaptation of \citet{harwath2016unsupervised} which feeds a spectrogram of the speech signal to a convolutional architecture, or a hybrid architecture of convolutional and recurrent layers using  Mel-Frequency Cepstral Coefficient (MFCC) features from the audio signal as input.

Models of grounded speech are mainly optimized for and evaluated on image retrieval from spoken caption and vice versa. But a range of diagnostic analyses have also been performed on the hidden representations of these models to study whether they encode knowledge about the identity and boundaries of subword units such as phonemes and syllables \cite{alishahi-etal-2017-encoding, harwath2019towards, khorrami_2021} as well as individual words \cite{chrupala-etal-2017-representations,havard2019word}. Moreover, in addition to examining form-meaning associations at the utterance level, \cite{harwath2017learning} explicitly learn a lexicon by extracting audio and image segments, clustering each modality separately, and mapping them together by calculating the pairwise similarities of their members in the joint semantic space.

\paragraph{Spoken language grounded in video.}
There have been limited attempts at learning spoken language grounded in video instead of static images. 
\citet{boggust2019grounding} sample audio-visual fragments from cooking videos, however their grounded model treats video frames as still images and discard their temporal order.
%The loose synchrony between the two modalities, such that objects may be mentioned in the audio at a
%different point in time than they occur in the video, remains the main challenge for this approach.
\citet{rouditchenko2020avlnet} integrate the temporal information when encoding videos from the Howto100m dataset \cite{miech2019howto100m}, and perform better than previous work in language and video clip retrieval. 
%Rouditchenko et al. (2021) present an architecture (AVLNet) which does model the
%time dimension in the video stream: the network consists of an audio encoder (ResNet-based), a video
%encoder which combines 3D and 2D modeling (also ResNet-based), as well as an optional text encoder.
%This architecture is trained with a contrastive loss on randomly sampled audio-video fragments from the
%Howto100m dataset (Miech et al., 2019) consisting of 136 million video clips sourced from 1.22 million
%narrated instructional web videos. The model is evaluated on the video clip and language retrieval tasks
%on smaller video datasets annotated with clip boundaries and text summaries, and is shown to outperform
%previously proposed models of Arandjelovic and Zisserman (2018) and Boggust et al. (2019). The model
%also transfers to the image-audio retrieval setting. Qualitative analysis suggests that the model aligns
%semantically related audio and visual features to particular dimensions of the embedding space
But models trained on such instructional video datasets often do not generalize well to other domains. \citet{monfort2021spokenmoments} highlight this limitation and show that training on their larger and more diverse Spoken Moments in Time dataset leads to better generalization.
Still, the point remains that most of these video datasets focus on cases where there is a strong correlation between the spoken descriptions and their background visual context, a characteristic that is not representative of the experience of learning language in real world.

\paragraph{Child language learning from video.}
%\todo[inline]{I'm still cleaning up the section on infant language learning from video.}
% In addition to anecdotes and news pieces on how watching cartoons affects chidlren's accent and vocabulary\footnote{Example of stories on how watching Peppa Pig has changed children's accent and word usage: \url{https://www.theguardian.com/tv-and-radio/2021/jul/19/peppa-pig-american-kids-british-accents}, \url{https://globalnews.ca/news/4961058/kids-accent-peppa-pig}.}, 
 There are many studies on young children learning language by watching videos; see \cite{vanderplank2010deja} for a survey. The main takeaway of these studies is that language learning is much more effective in a social, conversational setting than by passively watching videos \cite{kuhl2003foreign,anderson2005television,robb2009just} (and adding social interaction while watching videos improves learning; \citet{lytle2018two}), but learning does happen in such contexts. Importantly for our goal, techniques such as the intermodal preferential looking paradigm have been developed for this specific setting to systematically test young language learners' knowledge of words, syntactic structure and semantic roles \cite{hirsh1996intermodal,bergelson20126,noble2011comprehension}. \citet{nikolaus-fourtassi-2021-evaluating} employ this evaluation strategy to test semantic knowledge at word and sentence level in their computational model of word learning from images. We adapt this approach to evaluate how our grounded model associates semantic information to spoken words and utterances from video. 
 
 
%\citep[e.g.][]{synnaeve2014learning,harwath2015deep, harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.



% \paragraph{Audiovisual models}
% \citet{aytar2016soundnet,owens2016visually,owens2016ambient}
% \paragraph{Video captioning}
% \citet{krishna2017dense,zhou2018end}


