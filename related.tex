\section{Related Work}
\label{sec:related}

Early attempts at simulating grounded language learning focus on
interactions between adults and young children while playing with a
set of objects from different categories \cite{roy1999learning,
  roy2000grounded, roy2000learning, roy2002learning,
  gorniak2003visually, mukherjee2003visual}. In a representative study
from these series, \citet{roypentland2002learning} use speech recorded from
such interactions paired with different views of the visible objects
to identify linguistic units (i.e.\ words) and visual categories, and
to map these two modalities together. A hard-coded visual system
extracts object representations from images, and spoken utterances are
represented as phoneme probabilities generated by a RNN pre-trained on
spectrograms.  Their experiments on small-scale data (around 20 words
and seven visual categories) show that the model can segment words and
map them to visual categories.

\subsection{Spoken Language Grounded in Images}
\label{sec:images}
The availability of datasets of images associated with spoken captions
such as Flickr Audio Captions \citep{harwath2015deep}, Places
\cite{zhou2014learning} and Spoken COCO \citep{hsu2019transfer} led to
a rapid development of deep models of grounded language learning; see
\citet{chrupala-visually-2021} for a comprehensive overview. \todo{MN: maybe 
highlight here the difference of these models to the early approaches by deb 
roy (e.g. handling larger + more realistic data, ..)?}
 Most of
these models adapt the architecture of \citet{karpathy2014deep} by
implementing separate pathways for encoding the visual and speech
modality, and mapping these encodings into a joint representation
space. Visual features are extracted from a pre-trained
image classification model that processes the whole or a specific
region of an image (however see \citet{harwath2018jointly}, who train the
model end-to-end on images and their spoken captions on the Places
dataset). The audio encoder component in most models is 
either an adaptation of \citet{harwath2016unsupervised} which feeds a
spectrogram of the speech signal to a convolutional architecture, or a
hybrid architecture of convolutional followed by recurrent layers using
Mel-Frequency Cepstral Coefficient (MFCC) features from the audio
signal as input as introduced by \citet{chrupala-etal-2017-representations}.

Models of speech grounded in images are optimized for and evaluated on
image retrieval from spoken caption and vice versa. Additionally, a range of
diagnostic analyses have been performed on the hidden
representations of these models to study whether they encode knowledge
about the identity and boundaries of subword units such as phonemes
and syllables \cite{alishahi-etal-2017-encoding, harwath2019towards,
  khorrami_2021} as well as individual words
\cite{chrupala-etal-2017-representations,havard2019word}. Moreover, in
addition to examining form-meaning associations at the utterance
level, \citet{harwath2017learning} explicitly learn a lexicon by
extracting audio and image segments, clustering each modality
separately, and mapping them together by calculating the pairwise
similarities of their members in the joint semantic space.

\subsection{Spoken Language Grounded in Video}
\label{sec:video}
There have been also been recent attempts to learn spoken language grounded
in video instead of static images.  \citet{boggust2019grounding}
sample audio-visual fragments from cooking videos, however their
grounded model treats video frames as still images and discard their
temporal order.
% The loose synchrony between the two modalities, such that objects
% may be mentioned in the audio at a different point in time than they
% occur in the video, remains the main challenge for this approach.
\citet{rouditchenko2020avlnet} integrate the temporal information when
encoding videos from the Howto100m dataset \cite{miech2019howto100m},
and perform better than previous work in language and video clip
retrieval.
% Rouditchenko et al. (2021) present an architecture (AVLNet) which
% does model the time dimension in the video stream: the network
% consists of an audio encoder (ResNet-based), a video encoder which
% combines 3D and 2D modeling (also ResNet-based), as well as an
% optional text encoder.  This architecture is trained with a
% contrastive loss on randomly sampled audio-video fragments from the
% Howto100m dataset (Miech et al., 2019) consisting of 136 million
% video clips sourced from 1.22 million narrated instructional web
% videos. The model is evaluated on the video clip and language
% retrieval tasks on smaller video datasets annotated with clip
% boundaries and text summaries, and is shown to outperform previously
% proposed models of Arandjelovic and Zisserman (2018) and Boggust et
% al. (2019). The model also transfers to the image-audio retrieval
% setting. Qualitative analysis suggests that the model aligns
% semantically related audio and visual features to particular
% dimensions of the embedding space
Models trained on such instructional video datasets often do not
generalize well to other domains. \citet{monfort2021spokenmoments}
highlight this limitation and show that training on their larger and
more diverse Spoken Moments in Time dataset leads to better
generalization.  The point remains that these video datasets contain
descriptive speech, thus ensuring that there is a strong correlation
between the spoken language and their visual context, a characteristic
that is not representative of the experience of learning language in
the real world.

\paragraph{Child language learning from video.}
% \todo[inline]{I'm still cleaning up the section on infant language
% learning from video.}  In addition to anecdotes and news pieces on
% how watching cartoons affects chidlren's accent and
% vocabulary\footnote{Example of stories on how watching Peppa Pig has
% changed children's accent and word usage:
% \url{https://www.theguardian.com/tv-and-radio/2021/jul/19/peppa-pig-american-kids-british-accents},
% \url{https://globalnews.ca/news/4961058/kids-accent-peppa-pig}.},
There are many studies on young children learning language by watching
videos; see \citet{vanderplank2010deja} for a survey. The main takeaway
of these studies is that language learning is much more effective in a
social, conversational setting than by passively watching videos
\cite{kuhl2003foreign,anderson2005television,robb2009just},\footnote{Adding
social interaction while watching videos improves learning;
\citet{lytle2018two}.} but learning does happen in such
contexts. Importantly for our goal, techniques such as the intermodal
preferential looking paradigm have been developed to systematically test young 
language learners' knowledge of words, syntactic structure and semantic roles
\cite{hirsh1996intermodal,bergelson20126,noble2011comprehension}.
\citet{nikolaus-fourtassi-2021-evaluating}
employ this evaluation strategy to test semantic knowledge at word and
sentence level in their computational model of word learning from
images. We adapt this approach to evaluate how our grounded model
associates semantic information to spoken words and utterances from
video.
%The paradigm has been used in language acquisition research to
%evaluate children's early linguistic knowledge
%\citep[e.g.,][]{noble2011comprehension,bergelson20126}, by testing
%whether they can distinguish a matching (target) visual referent from
%a foil (distractor) referent when prompted with a word or
%sentence.
 
 
% \citep[e.g.][]{synnaeve2014learning,harwath2015deep,
% harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.



% \paragraph{Audiovisual models}
% \citet{aytar2016soundnet,owens2016visually,owens2016ambient}
% \paragraph{Video captioning}
% \citet{krishna2017dense,zhou2018end}
\todo{GC: I think we need a paragraph or so on recent work on
  self-supervised learning from multimodal data. SOme possible works
  to mention:
  https://arxiv.org/abs/2104.11178, https://arxiv.org/abs/2103.03206}
