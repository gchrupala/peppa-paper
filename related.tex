\section{Related work}
\label{sec:related}

Early attempts at simulating grounded language learning focus on interactions between adults and young children while playing with a set of objects from different categories \cite{roy1999learning, roy2000grounded, roy2000learning, roy2002learning, gorniak2003visually, mukherjee2003visual}. In a representative study from these series, \citet{roy2002learning} use speech recorded from such interactions paired with different views of the visible objects to identify linguistic units (i.e.\ words) and visual categories, and to map these two modalities. A built-in visual system extracts object representations from images, and spoken utterances are represented as phoneme probabilities generated by a pre-trained RNN. Their experiments show that the model can segment words and map them to visual categories in small scale (around 20 words and seven visual categories).

The availability of image-caption datasets such as MS-COCO \cite{lin2014microsoft} and Flickr30K \cite{plummer2015flickr30k} and their spoken counterparts such as Places \cite{zhou2014learning} and Speech COCO \cite{speech_coco} led to a rapid development of deep models of grounded language learning; see \citet{chrupala-visually-2021} for an overview. Most of these models adapt the architecture of \citet{karpathy2014deep} by implementing separate pathways for encoding the visual and linguistic modality, and mapping these encodings into a joint representation space. Many existing models extract visual features from a pre-trained image classification model that processes the whole or a specific region of an image (although \cite{harwath2018jointly} train the model end-to-end on images and their spoken captions from the Places dataset). The linguistic encodings are captured by a customized text or speech model.  The audio encoder component in most models is often either an adaptation of \citet{harwath2016unsupervised} which feeds a spectrogram of the speech signal to a convolutional architecture, or extract  Mel-Frequency Cepstral Coefficient (MFCC) features from the input signal and pass it to a few convolutional and recurrent layers. 

There have been limited attempts at learning spoken language grounded in video instead of static images. 
\citet{boggust2019grounding} sample audio-visual fragments from cooking videos, the grounded model treats video frames as still images and discard their temporal order.
%The loose synchrony between the two modalities, such that objects may be mentioned in the audio at a
%different point in time than they occur in the video, remains the main challenge for this approach.
\citet{rouditchenko2020avlnet} integrate the temporal information when encoding videos from the Howto100m dataset \cite{miech2019howto100m}, and perform better than previous work in language and video clip retrieval. 
%Rouditchenko et al. (2021) present an architecture (AVLNet) which does model the
%time dimension in the video stream: the network consists of an audio encoder (ResNet-based), a video
%encoder which combines 3D and 2D modeling (also ResNet-based), as well as an optional text encoder.
%This architecture is trained with a contrastive loss on randomly sampled audio-video fragments from the
%Howto100m dataset (Miech et al., 2019) consisting of 136 million video clips sourced from 1.22 million
%narrated instructional web videos. The model is evaluated on the video clip and language retrieval tasks
%on smaller video datasets annotated with clip boundaries and text summaries, and is shown to outperform
%previously proposed models of Arandjelovic and Zisserman (2018) and Boggust et al. (2019). The model
%also transfers to the image-audio retrieval setting. Qualitative analysis suggests that the model aligns
%semantically related audio and visual features to particular dimensions of the embedding space.
\todo{We need to make a clear comparison with the last two: what is our main contribution over theirs?}
However, models trained on such instructional video datasets often do not generalize well to other domains, a limitation highlighted by \citet{} who show that training on the more diverse Spoken Moments in Time dataset \cite{monfort2021spokenmoments} leads to better generalization. 

\todo[inline]{I'm still cleaning up the section on infant language learning from video.}
 
%\citep[e.g.][]{synnaeve2014learning,harwath2015deep, harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.



% \paragraph{Audiovisual models}
% \citet{aytar2016soundnet,owens2016visually,owens2016ambient}
% \paragraph{Video captioning}
% \citet{krishna2017dense,zhou2018end}


