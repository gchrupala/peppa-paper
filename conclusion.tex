\section{Conclusion}
\label{sec:conclusion}
We simulate grounded language learning in a naturalistic setting, where 
the connection between the linguistic and visual modalities is not always strong 
and is potentially confounded by correlations with non-semantic aspects of 
the speech signal. Our experimental results suggest that despite the 
challenges inherent to the naturalistic aspects of our training dataset, a 
simple bimodal architecture can capture aspects of visual meaning of individual 
words as well as full utterances, and generalize well to narrative utterances
featuring a single unseen speaker and a descriptive rather than
conversational style. Our analyses show that generalization is substantially
boosted by fine-tuning audio representations pretrained on unlabeled
single-modality speech data. Fine-tuning a pretrained video encoder
also makes a contribution, but is less crucial to generalization from
dialog to narration.
%
We also investigate the role of temporal information in learning form-meaning 
mappings and show that having access to 
time information facilitates learning, except for very short video segments. 

\subsection{Limitations and Future Work}
To better understand what aspects of language are learning in our
setting, we need to to carry out in-depth analyses of learned 
representations on sub-word, lexical, and phrasal levels. It would also be 
worthwhile to figure out the details of how specifically temporal information 
in video contributes to acquiring linguistic knowledge.  Some analyses in 
this direction are currently constrained by the size of the evaluation 
dataset, and more large-scale datasets are needed in the future.

We model the acquisition of spoken language from 
language-internal correlations as well as from grounding in vision 
by fine-tuning an audio encoder pretrained on read speech. This 
approach is rather simplistic and does not match the real experience of 
language learners. It would be interesting to make the setting
more realistic by using pretraining data which reflect a young
learner's experience more closely, and to realistically interleave learning via
self-supervision from speech and via grounding in vision.
Ideally we would want to dispense with supervised pretraining of 
the video encoder as well and rather use a model pretrained in a
self-supervised way also for this modality.

