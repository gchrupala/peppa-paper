\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage[nohyperref,acceptedWithA]{tacl2021v1}
\usepackage{etoolbox}
\usepackage[hidelinks=true]{hyperref}
\usepackage{appendix}
\usepackage{natbib}
\usepackage[utf8x]{inputenc}
\usepackage{booktabs}
%\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\usepackage{adjustbox}
%\usepackage{microtype}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{breqn}
\DeclareMathOperator*{\mean}{mean}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength\titlebox{7cm}
%\errorcontextlines=5
\begin{document}

\title{Learning English with Peppa Pig}

\author{Mitja Nikolaus\\
  Aix-Marseille University\\
  \texttt{mitja.nikolaus@univ-amu.fr}
  \And
  Afra Alishahi\\
  Tilburg University\\
  \texttt{a.alishahi@uvt.nl}
  \AND
  Grzegorz Chrupa≈Ça\\
  Tilburg University\\
  \texttt{grzegorz@chrupala.me}}

\date{}


\maketitle
\begin{abstract}
  Attempts to computationally simulate the acquisition of spoken
  language via grounding in perception have a long tradition
  but have gained momentum in the past few years.  Current neural
  approaches exploit associations between the spoken and
  visual modality and learn to represent speech and visual data
  in a joint vector space. A major unresolved issue from the point
  of ecological validity is the training data, typically consisting of
  images or videos paired with spoken descriptions of what is
  depicted. Such a setup guarantees an unrealistically strong
  correlation between speech and the visual world.  In
  the real world the coupling between the linguistic and the visual is
  loose, and often contains confounds in the form of correlations with
  non-semantic aspects of the speech signal. The current study is a
  first step towards simulating a naturalistic grounding scenario by
  using a dataset based on the children's cartoon {\it Peppa Pig}. We
  train a simple bi-modal architecture on the portion of the data
  consisting of naturalistic dialog between characters, and evaluate
  on segments containing descriptive narrations. Despite the weak and confounded
  signal in this training data our model succeeds at learning aspects
  of the visual semantics of spoken language.
\end{abstract}

\input{intro}
\input{related}
\input{method}
\input{results}
\input{conclusion}
\section*{Acknowledgements}
We would like to thank Nikos Papasarantopoulos and Shay B.\ Cohen for
creating the Peppa Pig annotations and for sharing them with us.
Part of the NWO/E-Science Center grant number 027.018.G03 was used to
purchase the video data on DVD. Thanks to Bertrand Higy for taking care of this
purchase, as well as for sharing his ideas with us in the initial stages of
this work. We also thank Abdellah Fourtassi for his useful feedback.
\bibliography{biblio,anthology}
\bibliographystyle{acl_natbib}
%\input{appendix}
\end{document}
