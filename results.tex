\section{Results}
\label{sec:results}
\paragraph{Performance metrics}
\Cref{tab:scores-dialog} and \Cref{tab:scores-narration} show
the performance of several model configurations on the retrieval and
triplet tasks on the dialog and narration datasets respectively.

In the case of the narration data this scores is not confounded by
speaker-based clues, which is a indication that the model possibly
learned to detect some aspects of utterance meaning. We investigate
this hypothesis further using multiple representational similarity
analysis.
 

 \begin{table}
   \centering
   \input{results/scores_dialog.tex}
   \caption{Retrieval and triplet scores on dialog validation data.}
   \label{tab:scores-dialog}
 \end{table}

\begin{table}
   \centering
   \input{results/scores_narration.tex}
   \caption{Retrieval and triplet scores on narration validation data.}
   \label{tab:scores-narration}
 \end{table}
 
 
\paragraph{Multiple representational similarity analysis}



The effect of
the {\tt samespeaker} predictor for the dialog data is negative and
small in size.  These results further indicate that the model learns
some aspects of word-level semantics as captured by GloVe word
vectors, and that speaker identity does not appear to be a substantial
impact on utterance embeddings.

Perhaps unexpectedly, the predictor meant to capture phonemic distance
{\tt distance} is not strongly associated with utterance similarity,
although it should be noted that here we are only investigating model
embeddings after the final attention pooling layer.The strength of
the association between differences in utterance duration {\tt
  durationdiff} and pairwise similarities apparent in this data was
suprising and possibly undesirable, and thus warrants further investigation.

