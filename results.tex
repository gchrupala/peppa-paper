\section{Results}
\label{sec:results}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{results/recall_at_1_to_n_test.pdf}
  \caption{Recall@$N$ as a function of $N$, for the narration test
    data. We show recall for the complete model, for the {\sc fixed} and {\sc jitter}
    retrieval evaluation settings. }
  \label{fig:recall_at_1_to_n}
\end{figure}

\begin{table}[htb]
  \input{results/scores_test.tex}
  \caption{Results of the full model on narration test
  	data. We show the mean and standard deviation of 
  	bootstrapped scores, pooled over four training runs
	(chance recall@10 = 10\%; chance triplet accuracy = 50\%).}
  \label{tab:test_scores}
\end{table}


\Cref{fig:recall_at_1_to_n} shows recall@$N$ for values of $N$
between 1 and 10 for the complete model on test narration data in both 
{\sc fixed} and {\sc jitter} conditions. Both plots show that the value of 
recall@$N$ increases monotnically. For the rest 
of this paper, we only report recall@10.

\Cref{tab:test_scores} presents the recall@10 and triplet accuracy
scores on test narration data obtained with the complete
model. In \Cref{sec:ablations} we investigate the impact
of various components of our training setup on performance as measured
by recall@10 and triplet accuracy.  In \Cref{sec:minimal-pairs} we
focus on the targeted evaluation via minimal pairs.


\subsection{Ablations}
\label{sec:ablations}
For completeness, we report results on both dialog and narration
data. However, the scores on narration are our main focus as they are
not confounded by speaker-based clues, and thus indicate to what
extent the model learns aspects of utterance meaning.

For experiments in Section~\ref{sec:pretraining} we include each run as a separate boxplot
to show the consistency of the results between runs in
different training conditions.  For the other experiments we collapse
the results of the four runs to avoid clutter.

\subsubsection{Pretraining and Fine-tuning}
\label{sec:pretraining}
\begin{figure*}[htb]
	\centering
	\includegraphics[width=\textwidth]{results/ablations/pretraining.pdf}
	\caption{Effect of pretraining on performance on the dialog
          and narration validation data. The top row shows recall@10
          (chance = 10\%); the bottom row triplet accuracy (chance =
          50\%). Within each condition, we show scores for four
          separate runs. AV: pretrained audio and video; A: pretrained
          audio; V: pretrained video; None: no pretraining.}
	\label{fig:pretraining}
      \end{figure*}

Results on different pretraining configurations are shown in
\Cref{fig:pretraining}.
The best overall performance on both the dialog and the narration data is 
achieved with a model where both the video and audio encoder are pre-trained 
before being fine-tuned on our data. On narration data, for both metrics,
we see a clear ranking of
configurations from best to worst: audio and video pretraining (AV), 
audio pretraining (A), video pretraining (V) and no training (None). 
Meanwhile for dialog data, the performance between A and V conditions
is comparable. In the absence of any pretraining (None),
some runs fail to converge, thus performing at chance level.

To further understand and disentangle the effects of audio pretraining and 
fine-tuning, we train a model with frozen parameters of the 
\textsc{wav2vec2} module. The effect of this condition is shown in \Cref{fig:freeze_wav2vec}.
\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{results/ablations/freeze_wav2vec.pdf}
  \caption{Effect of freezing the parameters of the \textsc{wav2vec2}
    module on model performance, on the dialog and narration
    validation data (True: \textsc{wav2vec2} frozen; False:
    \textsc{wav2vec2} trained). The top row
    shows recall@10; the bottom row triplet accuracy.}
  \label{fig:freeze_wav2vec}
\end{figure}
We find without fine-tuning of the \textsc{wav2vec2} module, performance 
decreases substantially on both metrics. In other words, best 
performance is only achieved with pre-trained and fine-tuned models.


\subsubsection{Jitter}
Next, we evaluate a model that has been trained with varying video and audio 
lengths (\textsc{jitter}). For fair comparison, here we report recall@10 for both 
\textsc{fixed} and \textsc{jitter} validation configurations.
As seen in \Cref{fig:jitter}, the effect of \textsc{jitter} is only
minor and that performance is comparable.
However, we do observe some 
performance improvements when using \textsc{jitter} in the
minimal pairs evaluation (cf. \Cref{sec:minimal-pairs}).
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\columnwidth]{results/ablations/jitter.pdf}
	\caption{Effect of jitter on model performance, on the dialog
          and narration validation data (True: jitter; False:
          fixed). The top row shows recall@10 on {\sc fixed}
          evaluation data; the bottom row on {\sc jitter}-ed data.}
	\label{fig:jitter}
\end{figure}



\subsubsection{Temporal Information}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{results/ablations/static.pdf}
  \caption{Effect of a \textsc{static} image encoder on model
    performance, on the dialog and narration validation data (True:
    static video encoder; False: regular video encoder). The top row
    shows recall@10; the bottom row triplet accuracy. For both conditions only
    the audio modality is pretrained.}
  \label{fig:static}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\columnwidth]{results/duration_effect.pdf}
  \caption{The effect of clip duration on the difference in
    performance between models with and without access to temporal
    information, on triplet data. Here we calculate the 
    undiscretized triplet scores (i.e.\ $\mathrm{cosine}(A, P) - \mathrm{cosine}(A,
    N))$, average them over all triplets with the same duration, and
    for each duration compute the difference in the average between
    time-aware and static models. The size of the points corresponds
    to the number of triplets within each duration. The line of fit is
    a {\sc loess} smoother weighted by size.}
  \label{fig:duration_effect}
\end{figure}
\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{results/ablations/scrambled_video.pdf}
	\caption{Effect of scrambling the video frames on model performance, on the 
	dialog and narration validation data (True: video frames scrambled;
        False: video frames in order). The top row shows recall@10;
		the bottom row triplet accuracy.}
	\label{fig:scrambled_video}
      \end{figure}
Finally, we explore the role of the temporal nature of the visual
modality.  \Cref{fig:static} compares the model with the regular video
encoder with one using the \textsc{static} baseline encoder.  For this
comparison we did not pretrain the video encoder in either condition,
in order to remove the confound of the pretraining data.\footnote{Note
  that there is one further confound we do not control for: the
  regular encoder has many more parameters than the {\sc static} one (31.5M vs. 11.7M).}
Across all
metrics, we observe substantial performance drops for the
\textsc{static} model, which has access to the same video frames, but
does not have access to their temporal ordering.

Additionally we investigate the effect of clip duration on this same
comparison, using the triplet evaluation
data. \Cref{fig:duration_effect} shows that the effect is nonlinear,
and for the shortest clips temporal information does not
help and may even have a detrimental effect.

\Cref{fig:scrambled_video} shows the effect of scrambling the video frames 
along the temporal dimension at test time (note that here the video encoders 
are pretrained). As expected, we observe substantial 
performance drops when the model does not see the video frames in 
the correct order. 

For this ablation the differential impact of clip duration on
the two conditions is very similar as in the {\sc static}
ablation (figure not included).

\subsection{Minimal Pairs}
\label{sec:minimal-pairs}
\begin{table*}[htb]
	\centering
	\input{results/targeted_triplets/minimal_pairs.tex}
	\caption{Minimal pair accuracies for nouns and verbs for different model 
		ablations. W2V Finet: \textsc{wav2vec2} module finetuned; A Pretr: 
		Audio encoder pretrained; V Pretr: Video encoder pretrained; Tmp 
		Enc: Video encoder with temporal information (not \textsc{static}); 
		Tmp Frames: Video frames in correct temporal order (not scrambled). 
		Mean and standard 
		deviation calculated over bootstrapped scores (100 re-samples), pooled 
		over 4 training runs.}
	\label{tab:minimal_pair_results}
\end{table*}
\begin{figure}[htb]
  \centering
  \includegraphics[width=.5\textwidth]{results/targeted_triplets/acc_per_word_NOUN.pdf}
  \caption{Per-word accuracies on the minimal pairs evaluation data for nouns.}
  \label{fig:accuracy_targeted_triplets_nouns}
\end{figure}
\begin{figure}[htb]
	\centering
	\includegraphics[width=.5\textwidth]{results/targeted_triplets/acc_per_word_VERB.pdf}
	\caption{Per-word accuracies on the minimal pairs evaluation data for 
	verbs.}
	\label{fig:accuracy_targeted_triplets_verbs}
\end{figure}


\Cref{tab:minimal_pair_results} presents results for the minimal pair 
evaluation along with several ablations. Models which are 
pretrained and fine-tuned with \textsc{jitter} (row 0) perform best. In the 
first two configurations (rows 0 and 1), there is not much difference in the scores for 
verbs and nouns. However, we observe a substantial performance drop for both nouns and verbs if 
the \textsc{wav2vec2} module is not fine-tuned.

If the model is trained without \textsc{jitter} (row 2), performance drops substantially
for nouns, but not for verbs. One possible explanation for this could be that 
the evaluation samples for nouns are on average shorter than those for verbs 
(nouns: $0.43s$ vs. verbs: $0.49s$), and a model trained with \textsc{jitter} 
performs better on short clips because it has been exposed to clips of varying 
duration during training. Supporting this hypothesis, we find a positive 
correlation between log duration of clips and accuracy, which is lower for 
models trained with \textsc{jitter} (Pearson $r= 0.52$, $p < 0.001$) than for 
models without \textsc{jitter} (Pearson $r= 0.69$, $p < 0.001$).

In line with the general results, we find that the benefit of audio pretraining 
(row 5) is greater than that of video pretraining (row 4). A model without any 
pretraining (row 3) only performs marginally above chance.

For a model trained with a \textsc{static} video encoder (row 6), we compare 
performance to a model that was also trained without video pretraining (row 5) 
as done for the general results. We observe a slight performance 
improvement for nouns, and no significant difference for
verbs.
We suspect that temporal information is not crucial for the minimal pairs evaluation, 
because most evaluation samples are clips of short duration (on average: 
$0.44s$, i.e.\ 4-5 frames), thus limiting the benefit of the time dimension.
As we saw in the analysis of clip duration (\Cref{fig:duration_effect}), 
temporal information for such short clips does not improve performance, and 
could even have detrimental effects. In the alternative temporal ablation 
with scrambled video frames (row 7), we observe no significant performance 
drop compared to the base condition (row 0).

Figures \ref{fig:accuracy_targeted_triplets_nouns} and 
\ref{fig:accuracy_targeted_triplets_verbs} show per-word
accuracy for nouns and verbs for the best performing model configuration.
We observe substantial variance in the accuracy scores, suggesting that the 
difficulty to learn certain words varies. For example, the 
scores for \textit{house}, \textit{car}, and \textit{cake} are the lowest. This could be 
because these concepts are not easy to ground, either because they are used in 
displaced speech or because they do not often refer to a similar visual entity. 
When looking at our evaluation samples, we find that indeed the word \textit{house} 
is used in varying visual contexts (house entrance, whole house, inside the
house, rabbit's house) and in displaced speech (talking about going 
to somebody's house). Cars are only sometimes completely visible, often we see 
only cartoon characters \textit{in} a car. Regarding \textit{cake}, it refers to either a 
whole cake, a slice, dough, or crumbs.

On the other end, performance for concrete words such as \textit{ice}, 
\textit{box}, 
and \textit{sand} is the best, and indeed we find that in the evaluation examples 
these concepts are always present in the corresponding video and visually 
highly similar. Additionally, the words \textit{Pedro}, and \textit{Rebecca} 
are learned very well: They refer to \textit{Pedro Pony} and \textit{Rebecca Rabbit}, 
easily visually distinguishable from characters belonging to other species.

Further investigations with larger datasets are necessary to reveal the 
underlying reasons for difficulty, and relating them to predictors of age of 
acquisition in the child language acquisition literature 
\cite{roy2015predicting,frank2021variability}. 


