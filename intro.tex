\section{Introduction}
\label{sec:intro}

Attempts to model or simulate the acquisition of spoken language via
grounding in the visual modality date to the beginning of this century
\citep{roypentland2002learning} but have gained momentum recently
with the revival of neural networks
\citep[e.g.][]{synnaeve2014learning,harwath2015deep,
  harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.
Current approaches work well enough from an applied point of view, 
but most are not generalizable to real-life situations that humans or 
adaptive artificial agents experience. Commonly used training data
consist of images or videos paired with spoken descriptions
of the scene depicted: however, the type of input that a language learner receives 
from the environment is much more challenging.  
Firstly, speech is only loosely coupled with the visual modality in naturalistic settings
 \citep{matusevych2013automatic, beekhuizen2013word}. Speakers often mention 
 concepts that are not present in the immediate perceptual context, 
 or talk about events that are remote in space and/or time (for
 example past experiences or future plans).
 
Secondly, in addition to
correlations between the visual scenes and the {\it meaning} of spoken
utterances, there are also correlations with non-semantic aspects of
the speech signal, such as the voices of specific speakers, as well
as with non-speech ambient sounds. Although it is plausible that such
non-semantic correlations can sometimes be useful to the learner in
the general endeavor of making sense of the world, for the specific
task of learning the semantics of linguistic units they are likely more
often an obstacle, as they make it harder to zoom in on the
meaning-bearing aspects of the audio signal.

In the current study we make a first step towards simulating the
acquisition of language via grounding in perception in a more
naturalistic scenario.  Our main focus is on learning the meaning of
linguistic expressions from spoken utterances grounded in video.  We
use the well-known children's cartoon {\it Peppa Pig} as a case
study. Compared to commonly used video datasets, this dataset has a
number of interesting characteristics.  The visual modality is very
schematic, the language is simple in terms of vocabulary size and
syntactic complexity, and analysis of its linguistic features suggests
its suitability for beginner learners of English
\cite{kokla2021peppa,scheffler2021peppa}.  Crucially, however, most of
the speech in the videos consists of naturalistic dialogs between the
characters in which they do not only discuss the here and now, but
also often use displaced language.\footnote{For example, when Daddy
  Pig explains that they need to clean up before Mummy Pig sees the mess
  Peppa and George made, or when talking about plans to visit
  friends.}  Thus, the utterances are only loosely and noisily
correlated to the scenes and actions depicted in the videos.

This choice of data thus allows us to directly address the ecological limitations 
of the current approaches. In addition, the cartoon videos also contain 
comments interjected by the narrator. We use these for evaluating the 
acquisition of meaning as they are more descriptive and less noisy and allow 
us to measure performance, while controlling for speaker characteristics.

We implement a simple bi-modal architecture which learns spoken
language embeddings from videos, and train it on the Peppa Pig dataset.
Our contributions are the following:
\begin{itemize}
\item We evaluate model performance in terms of video fragment
  retrieval and additionally design controlled evaluation
  protocols inspired by the intermodal preferential looking
  paradigm \citep{hirsh1996intermodal};
\item We carry out ablations of model components in order to
  understand the effects of pre-training for the audio and video
  encoders, the role of temporal information, and of segmentation
  strategies while training. 
\end{itemize}
We show that despite the challenges of our naturalistic training data,
our model succeeds at learning associations between the form of spoken 
utterances and their visual semantics. Moreover, even though the model 
rarely hears words in isolation, it captures aspects of the visual meaning 
of frequent nouns and verbs.
Our ablation studies suggest that temporal information contributes
to video modeling (especially for longer segments), and that self-supervised pre-training
followed by fine-tuning of the audio encoder is key to the best
performance.



