\section{Introduction}
\label{sec:intro}

Attempts to model or simulate the acquisition of spoken language via
grounding in the visual modality date to the beginning of this century
\citep{roypentland2002learning} but have gained momentum recently
with the revival of neural networks
\citep[e.g.][]{synnaeve2014learning,harwath2015deep,
  harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.
Current approaches work well enough from an applied point of view, 
but most are not generalizable to real-life situations that humans or 
adaptive artificial agents experience. Commonly used training data
consist of images or videos paired with spoken descriptions
of the scene depicted, but the type of input that a language learner receives 
from its environment is much more challenging.  Firstly, speech is only
loosely coupled with the visual modality. Secondly in addition to
correlations between the visual scenes and the {\it meaning} of spoken
utterances, there are also correlations with non-semantic aspects of
the speech signal, such as the voice of specific speakers, as well
as with non-speech ambient sounds. Although it is plausible that such
non-semantic correlations can sometimes be useful to the learner in
the general endeavour of making sense of the world, for the specific
task of learning the semantics of linguistic units they are likely more
often an obstacle, as they make it harder to zoom in on the
meaning-bearing aspects of the audio signal.

In the current study we make a first step towards simulating the
acquisition of language via grounding in perception in a more naturalistic
scenario.  Our main focus is on learning the meaning of individual words 
as well as multi-word expressions from spoken utterances grounded in video.  
We use the well-known children's cartoon {\it Peppa Pig} as
a case study as a source of training and evaluation data. Compared to
commonly used video datasets, this data has a number of interesting
characteristics.  The visual modality is very schematic,  the
language is simple in terms of vocabulary size and syntactic
complexity, and analysis of its linguistic features suggests its suitability for beginner learners of English \cite{kokla2021peppa,scheffler2021peppa}.
Crucially, however, most of the speech in the videos
consists of naturalistic dialogs between the characters. The
utterances are only loosely and noisily correlated to the scenes and
actions depicted in the videos.

This choice of data thus allows us to directly address the ecological limitations 
of the current approaches. In addition, the cartoon videos also contain 
comments interjected by the narrator. We use these for evaluating the 
acquisition of meaning as they are more descriptive and less noisy and allow 
us to measure performance while controlling for speaker characteristics.

%One further aspect of learning spoken language via visual grounding is
%the fact that grounding is only part of the story. Human children
%arguably infer substantial amounts of information about language
%structure and meaning from purely linguistic co-occurrence
%statistics. A similar mechanism is what allows written language models
%such as BERT \citep{devlin-etal-2019-bert} or GPT-3 \citep{brown2020language} to capture and exhibit relatively sophisticated
%linguistic knowledge. Loosely similar approaches are starting to also
%make an impact for the spoken modality
%\citep[e.g.][]{wav2vec2,hsu2021hubert}. Here we take a simple
%pre-training based approach to integrating this type of
%self-supervision with learning-via-grounding.

Our contributions are the following:
\begin{itemize}
\item We implement a simple bi-modal architecture which learns
  spoken language embeddings from videos;
\item We evaluate model performance in terms of video fragment
  retrieval and additionally design controlled evaluation
  protocols inspired by the intermodal preferential looking
  paradigm \citep{hirsh1996intermodal};
\item We carry out ablations of model components in order to
  understand the effects of pre-training for the audio and video
  encoders, the role of temporal information, and of segmentation
  strategies while training. 
\end{itemize}
We show that despite the challenges of our naturalistic training data
our model succeeds at learning associations between the form of spoken 
utterances and their visual semantics. Moreover, even though the model 
rarely hears words in isolation, it captures aspects of the visual meaning 
of frequent nouns and verbs.
%
Our ablation studies suggest that temporal information contributes
substantially to video modeling, and that unsupervised pre-training
followed by fine-tuning of the audio encoder is key to the best
performance.



