\section{Introduction}
\label{sec:intro}

Attempts to model or simulate the acquisition of spoken language via
grounding in the visual modality date to the beginning of this century
\citep{roypentland2002learning} but have gained momentum since 2014
with the revival of neural networks
\citep[e.g.][]{synnaeve2014learning,harwath2015deep,
  harwath2016unsupervised,chrupala-etal-2017-representations,alishahi-etal-2017-encoding,harwath2018jointly,Merkx2019,havard2019models,rouditchenko2020avlnet,khorrami_2021,peng2021fastslow}.
Current approaches work well enough from an applied point of view but
leave much to be desired as regards ecological validity. Training data
typically consist of images or videos paired with spoken descriptions
of the scene depicted. The type of input that a child faces when
learning a language is much more challenging.  Firstly, speech is only
loosely coupled with the visual modality. Secondly in addition to
correlations between the visual scenes and the {\it meaning} of spoken
utterances, there are also correlations with non-semantic aspects of
the speech signal, such as the voice of specific characters\todo{MN: rather, 
more general: persons/ speakers?}, as well
as with non-speech ambient sounds. Always\todo{MN: Always = Even though?} it is 
plausible that such
non-semantic correlations can sometimes be useful to the learner in
the general endeavor of making sense of the world, for the specific
task of learning the semantics of linguistic units they are likely more
often an obstacle, as they make it harder to zoom in on the meaning
bearing aspects of the audio signal.

In the current study we make a first step towards simulating the
acquisition of language via grounding in perception in a more naturalistic
scenario.  We use the well-known children's cartoon {\it Peppa Pig} as
a case study as a source of training and evaluation data. Compared to
commonly used video datasets, this data has a number of interesting
characteristics.  The visual modality is very schematic, and the
language is also simple in terms of vocabulary size and syntactic
complexity. Crucially, however, most of the speech in the videos
consists of naturalistic dialogs between the characters. The
utterances are only loosely and noisily correlated to the scenes and
actions depicted in the videos.\todo{GC: Can we support this assertion
  in a quantitative way? MN: Difficult I think. To measure semantic correlation 
  between images/videos and speech we need to pass them through a model. So we 
  could run our model on a different dataset (e.g. spoken captions) and show 
  that it performs better there, but we wouldn't have directly comparable 
  evaluation metrics..}

This choice of data thus allows us to
directly address the ecological limitations of the current
approaches. In addition, the cartoon videos also contain comments
interjected by the narrator. We use these for a evaluation a source of
more descriptive and less noisy data which allows us to measure
performance while controlling for speaker characteristics.
Our contributions are the following:
\begin{itemize}
\item We implement a simple bi-modal architecture which learns
  spoken language embeddings from videos;
\item We evaluate model performance in terms of video fragment
  retrieval and additionally design controlled evaluation
  protocols inspired by the intermodal preferential looking
  paradigm \citep{hirsh1996intermodal};
\item We carry out ablations of model components in order to
  understand the effects of pre-training for the audio and video
  encoders, the role of temporal information, and of segmentation
  strategies while training. 
\end{itemize}
We show that despite the challenges of our naturalistic training data
our model succeeds at learning aspects the visual semantics of spoken
language. Our findings include the fact that temporal information
contributes substantially to video modeling, and that unsupervised
pre-training of the audio encoder is key to the best performance, but that
even the model trained completely from scratch on about 10 hours of
cartoon data performs substantially above chance.



