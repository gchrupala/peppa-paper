@inproceedings{tran2018closer,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={6450--6459},
  year={2018}
}

@inproceedings{wav2vec2,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12449--12460},
 publisher = {Curran Associates, Inc.},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{papasarantopoulos2021narration,
    title={Narration Generation for Cartoon Videos},
    author={Nikos Papasarantopoulos and Shay B. Cohen},
    year={2021},
    eprint={2101.06803},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    note={Preprint: \url{https://arxiv.org/abs/2101.06803}}
}

@misc{chrupala-visually-2021,
  title={Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques},
  author={Grzegorz Chrupa{\l}a},
  note={Preprint: \url{https://arxiv.org/abs/2104.13225}},
  year={2021}
  }
  
@misc{peng2021fastslow,
    title={Fast-Slow Transformer for Visually Grounding Speech},
    author={Puyuan Peng and David Harwath},
    year={2021},
    eprint={2109.08186},
    archivePrefix={arXiv},
    primaryClass={eess.AS}
}
@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={arXiv preprint arXiv:1908.02265},
  year={2019}
}

@misc{1993STIN...9327403G,
 adsnote = {Provided by the SAO/NASA Astrophysics Data System},
 adsurl = {https://ui.adsabs.harvard.edu/abs/1993STIN...9327403G},
 author = {{Garofolo}, J.~S. and {Lamel}, L.~F. and {Fisher}, W.~M. and {Fiscus}, J.~G. and {Pallett}, D.~S.},
 howpublished = {NASA STI/Recon Technical Report N},
 keywords = {Cd-Rom, Data Bases, Linguistics, Phonemes, Phonetics, Speech, English Language, Speech Recognition, Verbal Communication, Words (Language), Communications and Radar},
 pages = {27403},
 title = {{DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1}},
 year = {1993}
}

@inproceedings{adi2016fine,
 author = {Yossi Adi and
Einat Kermany and
Yonatan Belinkov and
Ofer Lavi and
Yoav Goldberg},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/AdiKBLG17.bib},
 booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction
Tasks},
 url = {https://openreview.net/forum?id=BJh6Ztuxl},
 year = {2017}
}

@unpublished{anonymous2021discrete,
 author = {Anonymous},
 journal = {OpenReview Preprint},
 note = {Anonymous preprint \url{https://openreview.net/forum?id=ShVI_e0NuB4}},
 title = {Discrete representations in neural models of spoken language},
 year = {2021}
}

@inproceedings{aytar2016soundnet,
 author = {Yusuf Aytar and
Carl Vondrick and
Antonio Torralba},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/AytarVT16.bib},
 booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain},
 editor = {Daniel D. Lee and
Masashi Sugiyama and
Ulrike von Luxburg and
Isabelle Guyon and
Roman Garnett},
 pages = {892--900},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {SoundNet: Learning Sound Representations from Unlabeled Video},
 url = {https://proceedings.neurips.cc/paper/2016/hash/7dcd340d84f762eba80aa538b0c527f7-Abstract.html},
 year = {2016}
}

@inproceedings{azuh2019towards,
 author = {Azuh, Emmanuel and Harwath, David and Glass, James},
 booktitle = {Interspeech},
 title = {Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio},
 year = {2019}
}

@inproceedings{baker2010lucid,
 author = {Baker, Rachel and Hazan, Valerie},
 booktitle = {DiSS-LPSS Joint Workshop 2010},
 title = {LUCID: a corpus of spontaneous and read clear speech in British English},
 year = {2010}
}

@inproceedings{belinkov2017analyzing,
 author = {Yonatan Belinkov and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BelinkovG17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {2441--2451},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Analyzing Hidden Representations in End-to-End Automatic Speech Recognition
Systems},
 url = {https://proceedings.neurips.cc/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html},
 year = {2017}
}

@article{Belinkov_2019,
 author = {Belinkov, Yonatan and Ali, Ahmed and Glass, James},
 doi = {10.21437/interspeech.2019-2599},
 journal = {Interspeech 2019},
 publisher = {ISCA},
 title = {Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition},
 url = {http://dx.doi.org/10.21437/interspeech.2019-2599},
 year = {2019}
}

@article{bengio2013estimating,
 author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
 journal = {arXiv preprint arXiv:1308.3432},
 title = {Estimating or propagating gradients through stochastic neurons for conditional computation},
 year = {2013}
}

@article{bernardi2016automatic,
 author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
 journal = {Journal of Artificial Intelligence Research},
 pages = {409--442},
 title = {Automatic description generation from images: A survey of models, datasets, and evaluation measures},
 volume = {55},
 year = {2016}
}

@inproceedings{boggust2019grounding,
 author = {Boggust, Angie W and Audhkhasi, Kartik and Joshi, Dhiraj and Harwath, David and Thomas, Samuel and Feris, Rog{\'e}rio Schmidt and Gutfreund, Danny and Zhang, Yang and Torralba, Antonio and Picheny, Michael and others},
 booktitle = {CVPR Workshops},
 pages = {29--32},
 title = {Grounding Spoken Words in Unlabeled Video.},
 year = {2019}
}

@article{chorowski_unsupervised_2019,
 abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
 author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, A{\"a}ron},
 doi = {10.1109/TASLP.2019.2938863},
 file = {IEEE Xplore Abstract Record:/home/bjrhigy/Zotero/storage/IPQ2QKKY/8822475.html:text/html;IEEE Xplore Full Text PDF:/home/bjrhigy/Zotero/storage/ZISUFIFF/Chorowski et al. - 2019 - Unsupervised Speech Representation Learning Using .pdf:application/pdf},
 issn = {2329-9304},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 keywords = {learning (artificial intelligence), neural nets, Neural networks, unsupervised learning, Feature extraction, Gaussian processes, speech processing, Decoding, Task analysis, vector quantisation, acoustic signal processing, Speech processing, acoustic unit discovery, Autoencoder, autoencoder models, autoencoding neural networks, discrete vector quantized VAE, Gaussian variational autoencoder, high capacity WaveNet decoder, high level semantic content, latent representation, latent representations, low level details, phoneme identities, phonetic content, Phonetics, pitch contour, Prototypes, simple dimensionality reduction bottleneck, speech representation learning, speech waveforms, unsupervised extraction, unsupervised speech representation, VQ-VAE, WaveNet autoencoders, unsupervised unit discovery, zerospeech 2017},
 note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 number = {12},
 pages = {2041--2053},
 title = {Unsupervised {Speech} {Representation} {Learning} {Using} {WaveNet} {Autoencoders}},
 volume = {27},
 year = {2019}
}

@misc{chung2014empirical,
 archiveprefix = {arXiv},
 author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
 eprint = {1412.3555},
 primaryclass = {cs.NE},
 title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
 year = {2014}
}

@article{chung2020vector,
 author = {Chung, Yu-An and Tang, Hao and Glass, James},
 journal = {arXiv preprint arXiv:2005.08392},
 title = {Vector-Quantized Autoregressive Predictive Coding},
 year = {2020}
}

@article{cleveland1979robust,
 author = {Cleveland, William S},
 journal = {Journal of the American statistical association},
 number = {368},
 pages = {829--836},
 publisher = {Taylor \& Francis},
 title = {Robust locally weighted regression and smoothing scatterplots},
 volume = {74},
 year = {1979}
}

@book{cover1999elements,
 author = {Cover, Thomas M},
 publisher = {John Wiley \& Sons},
 title = {Elements of information theory},
 year = {1999}
}

@inproceedings{davis2020discourse,
 address = {Online},
 author = {Davis, Forrest  and
van Schijndel, Marten},
 booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
 doi = {10.18653/v1/2020.conll-1.32},
 pages = {396--407},
 publisher = {Association for Computational Linguistics},
 title = {Discourse structure interacts with reference but not syntax in neural language models},
 url = {https://www.aclweb.org/anthology/2020.conll-1.32},
 year = {2020}
}

@inproceedings{deng2009imagenet,
 author = {Jia Deng and
Wei Dong and
Richard Socher and
Li{-}Jia Li and
Kai Li and
Fei{-}Fei Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/DengDSLL009.bib},
 booktitle = {2009 {IEEE} Computer Society Conference on Computer Vision and Pattern
Recognition {(CVPR} 2009), 20-25 June 2009, Miami, Florida, {USA}},
 doi = {10.1109/CVPR.2009.5206848},
 pages = {248--255},
 publisher = {{IEEE} Computer Society},
 timestamp = {Fri, 27 Mar 2020 00:00:00 +0100},
 title = {ImageNet: {A} large-scale hierarchical image database},
 url = {https://doi.org/10.1109/CVPR.2009.5206848},
 year = {2009}
}

@inproceedings{dindo2010probabilistic,
 author = {Dindo, Haris and Zambuto, Daniele},
 booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 organization = {IEEE},
 pages = {790--796},
 title = {A probabilistic approach to learning a visually grounded language model through human-robot interaction},
 year = {2010}
}

@article{dunbar2019zero,
 author = {Dunbar, Ewan and Algayres, Robin and Karadayi, Julien and Bernard, Mathieu and Benjumea, Juan and Cao, Xuan-Nga and Miskic, Lucie and Dugrain, Charlotte and Ondel, Lucas and Black, Alan W and others},
 journal = {arXiv preprint arXiv:1904.11469},
 title = {The zero resource speech challenge 2019: TTS without T},
 year = {2019}
}

@inproceedings{ebert2020visuospatial,
 address = {Barcelona, Spain (Online)},
 author = {Ebert, Dylan  and
Pavlick, Ellie},
 booktitle = {Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics},
 pages = {143--153},
 publisher = {Association for Computational Linguistics},
 title = {A Visuospatial Dataset for Naturalistic Verb Learning},
 url = {https://www.aclweb.org/anthology/2020.starsem-1.16},
 year = {2020}
}

@inproceedings{eloff2019multimodal,
 author = {Ryan Eloff and
Herman A. Engelbrecht and
Herman Kamper},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/EloffEK19.bib},
 booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
{ICASSP} 2019, Brighton, United Kingdom, May 12-17, 2019},
 doi = {10.1109/ICASSP.2019.8683587},
 pages = {8623--8627},
 publisher = {{IEEE}},
 timestamp = {Sun, 30 Jun 2019 01:00:00 +0200},
 title = {Multimodal One-shot Learning of Speech and Images},
 url = {https://doi.org/10.1109/ICASSP.2019.8683587},
 year = {2019}
}

@article{eloff_unsupervised_2019,
 abstract = {For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.},
 author = {Eloff, Ryan and Nortje, Andr{\'e} and van Niekerk, Benjamin and Govender, Avashna and Nortje, Leanne and Pretorius, Arnu and van Biljon, Elan and van der Westhuizen, Ewald and van Staden, Lisa and Kamper, Herman},
 file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/68I73Y4W/Eloff et al. - 2019 - Unsupervised acoustic unit discovery for speech sy.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/SGTC28RP/1904.html:text/html},
 journal = {arXiv:1904.07556 [cs, eess]},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2019},
 note = {arXiv: 1904.07556},
 title = {Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks},
 url = {http://arxiv.org/abs/1904.07556},
 urldate = {2020-06-24},
 year = {2019}
}

@article{fereidooni2020understanding,
 author = {Fereidooni, Sam and Mocz, Viola and Radev, Dragomir and Chun, Marvin},
 journal = {bioRxiv},
 publisher = {Cold Spring Harbor Laboratory},
 title = {Understanding and Improving Word Embeddings through a Neuroscientific Lens},
 year = {2020}
}

@inproceedings{girshick2014rich,
 author = {Ross B. Girshick and
Jeff Donahue and
Trevor Darrell and
Jitendra Malik},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/GirshickDDM14.bib},
 booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
 doi = {10.1109/CVPR.2014.81},
 pages = {580--587},
 publisher = {{IEEE} Computer Society},
 timestamp = {Sun, 02 Jun 2019 01:00:00 +0200},
 title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic
Segmentation},
 url = {https://doi.org/10.1109/CVPR.2014.81},
 year = {2014}
}

@inproceedings{gorniak2003visually,
 author = {Gorniak, Peter and Roy, Deb},
 booktitle = {Proceedings of the 5th international conference on Multimodal interfaces},
 pages = {219--226},
 title = {A visually grounded natural language interface for reference to spatial scenes},
 year = {2003}
}

@inproceedings{haghani_audio_2018,
 author = {Haghani, Parisa and Narayanan, Arun and Bacchiani, Michiel and Chuang, Galen and Gaur, Neeraj and Moreno, Pedro and Prabhavalkar, Rohit and Qu, Zhongdi and Waters, Austin},
 booktitle = {2018 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
 doi = {10.1109/SLT.2018.8639043},
 file = {IEEE Xplore Abstract Record:/home/bjrhigy/Zotero/storage/CKET9KPE/8639043.html:text/html;IEEE Xplore Full Text PDF:/home/bjrhigy/Zotero/storage/YWPP4D7R/Haghani et al. - 2018 - From Audio to Semantics Approaches to End-to-End .pdf:application/pdf},
 note = {ISSN: null},
 pages = {720--726},
 shorttitle = {From {Audio} to {Semantics}},
 title = {From {Audio} to {Semantics}: {Approaches} to {End}-to-{End} {Spoken} {Language} {Understanding}},
 year = {2018}
}

@inproceedings{harwath2015deep,
 author = {Harwath, David and Glass, James},
 booktitle = {2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
 organization = {IEEE},
 pages = {237--244},
 title = {Deep multimodal semantic embeddings for speech and images},
 year = {2015}
}

@inproceedings{harwath2016unsupervised,
 author = {David F. Harwath and
Antonio Torralba and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/HarwathTG16.bib},
 booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain},
 editor = {Daniel D. Lee and
Masashi Sugiyama and
Ulrike von Luxburg and
Isabelle Guyon and
Roman Garnett},
 pages = {1858--1866},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Unsupervised Learning of Spoken Language with Visual Context},
 url = {https://proceedings.neurips.cc/paper/2016/hash/82b8a3434904411a9fdc43ca87cee70c-Abstract.html},
 year = {2016}
}

@inproceedings{harwath2018jointly,
 author = {Harwath, David and Recasens, Adria and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
 booktitle = {Proceedings of the European conference on computer vision (ECCV)},
 pages = {649--665},
 title = {Jointly discovering visual objects and spoken words from raw sensory input},
 year = {2018}
}

@phdthesis{harwath2018learning,
 author = {Harwath, David Frank},
 school = {Massachusetts Institute of Technology},
 title = {Learning spoken language through vision},
 year = {2018}
}

@inproceedings{harwath2018vision,
 author = {David Harwath and
Galen Chuang and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/HarwathCG18.bib},
 booktitle = {2018 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2018, Calgary, AB, Canada, April 15-20, 2018},
 doi = {10.1109/ICASSP.2018.8462396},
 pages = {4969--4973},
 publisher = {{IEEE}},
 timestamp = {Tue, 18 Sep 2018 01:00:00 +0200},
 title = {Vision as an Interlingua: Learning Multilingual Semantic Embeddings
of Untranscribed Speech},
 url = {https://doi.org/10.1109/ICASSP.2018.8462396},
 year = {2018}
}

@inproceedings{harwath2019towards,
 author = {David Harwath and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/HarwathG19.bib},
 booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
{ICASSP} 2019, Brighton, United Kingdom, May 12-17, 2019},
 doi = {10.1109/ICASSP.2019.8682666},
 pages = {3017--3021},
 publisher = {{IEEE}},
 timestamp = {Tue, 23 Jul 2019 01:00:00 +0200},
 title = {Towards Visually Grounded Sub-word Speech Unit Discovery},
 url = {https://doi.org/10.1109/ICASSP.2019.8682666},
 year = {2019}
}

@article{harwath2020jointly,
 author = {Harwath, David and Recasens, Adri{\`a} and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
 journal = {International Journal of Computer Vision},
 number = {3},
 pages = {620--641},
 publisher = {Springer},
 title = {Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input},
 volume = {128},
 year = {2020}
}

@inproceedings{harwath2020learning,
 author = {David Harwath and
Wei{-}Ning Hsu and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HarwathHG20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Learning Hierarchical Discrete Linguistic Units from Visually-Grounded
Speech},
 url = {https://openreview.net/forum?id=B1elCp4KwH},
 year = {2020}
}

@misc{havard2017speech,
 author = {Havard, William and Besacier, Laurent and Rosec, Olivier},
 eprint = {1707.08435},
 note = {Preprint: \url{https://arxiv.org/abs/1707.08435}},
 title = {{SPEECH-COCO}: 600k visually grounded spoken captions aligned to {MSCOCO} data set},
 year = {2017}
}

@inproceedings{havard2019models,
 author = {William N. Havard and
Jean{-}Pierre Chevrot and
Laurent Besacier},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/HavardCB19.bib},
 booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
{ICASSP} 2019, Brighton, United Kingdom, May 12-17, 2019},
 doi = {10.1109/ICASSP.2019.8683069},
 pages = {8618--8622},
 publisher = {{IEEE}},
 timestamp = {Sun, 30 Jun 2019 01:00:00 +0200},
 title = {Models of Visually Grounded Speech Signal Pay Attention to Nouns:
{A} Bilingual Experiment on English and Japanese},
 url = {https://doi.org/10.1109/ICASSP.2019.8683069},
 year = {2019}
}

@inproceedings{havard2019word,
 address = {Hong Kong, China},
 author = {Havard, William N.  and
Chevrot, Jean-Pierre  and
Besacier, Laurent},
 booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},
 doi = {10.18653/v1/K19-1032},
 pages = {339--348},
 publisher = {Association for Computational Linguistics},
 title = {Word Recognition, Competition, and Activation in a Model of Visually Grounded Speech},
 url = {https://www.aclweb.org/anthology/K19-1032},
 year = {2019}
}

@inproceedings{havard2020catplayinginthesnow,
 address = {Online},
 author = {Havard, William  and
Besacier, Laurent  and
Chevrot, Jean-Pierre},
 booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
 doi = {10.18653/v1/2020.conll-1.22},
 pages = {291--301},
 publisher = {Association for Computational Linguistics},
 title = {Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech},
 url = {https://www.aclweb.org/anthology/2020.conll-1.22},
 year = {2020}
}

@inproceedings{he2016deep,
 author = {Kaiming He and
Xiangyu Zhang and
Shaoqing Ren and
Jian Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
 booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
 doi = {10.1109/CVPR.2016.90},
 pages = {770--778},
 publisher = {{IEEE} Computer Society},
 timestamp = {Wed, 17 Apr 2019 01:00:00 +0200},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://doi.org/10.1109/CVPR.2016.90},
 year = {2016}
}

@inproceedings{higy2020textual,
 address = {Online},
 author = {Higy, Bertrand  and
Elliott, Desmond  and
Chrupa{\l}a, Grzegorz},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.244},
 pages = {2698--2709},
 publisher = {Association for Computational Linguistics},
 title = {Textual {S}upervision for {V}isually {G}rounded {S}poken {L}anguage {U}nderstanding},
 url = {https://www.aclweb.org/anthology/2020.findings-emnlp.244},
 year = {2020}
}

@inproceedings{hsu2019transfer,
 author = {Wei-Ning Hsu and David Harwath and James Glass},
 booktitle = {Proc. Interspeech 2019},
 doi = {10.21437/Interspeech.2019-1227},
 pages = {3242--3246},
 title = {{Transfer Learning from Audio-Visual Grounding to Speech Recognition}},
 url = {http://dx.doi.org/10.21437/Interspeech.2019-1227},
 year = {2019}
}

@misc{hsu2020textfree,
 archiveprefix = {arXiv},
 author = {Wei-Ning Hsu and David Harwath and Christopher Song and James Glass},
 eprint = {2012.15454},
 note = {Preprint: \url{https://arxiv.org/abs/2012.15454}},
 primaryclass = {cs.CL},
 title = {Text-Free Image-to-Speech Synthesis Using Learned Segmental Units},
 year = {2020}
}

@article{hupkes2018visualisation,
 author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
 journal = {Journal of Artificial Intelligence Research},
 pages = {907--926},
 title = {Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
 volume = {61},
 year = {2018}
}

@inproceedings{kamper2017visually,
 author = {Kamper, Herman and Settle, Shane and Shakhnarovich, Gregory and Livescu, Karen},
 booktitle = {Interspeech},
 pages = {3677--3681},
 title = {Visually Grounded Learning of Keyword Prediction from Untranscribed Speech},
 year = {2017}
}

@inproceedings{kamper2018visually,
 author = {Kamper, Herman and Roth, Michael},
 booktitle = {Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages},
 pages = {253--257},
 title = {Visually Grounded Cross-Lingual Keyword Spotting in Speech},
 year = {2018}
}

@inproceedings{kamper2019semantic,
 author = {Herman Kamper and
Aristotelis Anastassiou and
Karen Livescu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/KamperAL19.bib},
 booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
{ICASSP} 2019, Brighton, United Kingdom, May 12-17, 2019},
 doi = {10.1109/ICASSP.2019.8683275},
 pages = {7120--7124},
 publisher = {{IEEE}},
 timestamp = {Sun, 30 Jun 2019 01:00:00 +0200},
 title = {Semantic Query-by-example Speech Search Using Visual Grounding},
 url = {https://doi.org/10.1109/ICASSP.2019.8683275},
 year = {2019}
}

@article{kamper2019semantic-ieee,
 author = {Kamper, Herman and Shakhnarovich, Gregory and Livescu, Karen},
 journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
 number = {1},
 pages = {89--98},
 publisher = {IEEE Press},
 title = {Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech},
 volume = {27},
 year = {2019}
}

@inproceedings{karpathy2014deep,
 author = {Andrej Karpathy and
Armand Joulin and
Fei{-}Fei Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KarpathyJL14.bib},
 booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada},
 editor = {Zoubin Ghahramani and
Max Welling and
Corinna Cortes and
Neil D. Lawrence and
Kilian Q. Weinberger},
 pages = {1889--1897},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
 url = {https://proceedings.neurips.cc/paper/2014/hash/84d2004bf28a2095230e8e14993d398d-Abstract.html},
 year = {2014}
}

@inproceedings{karpathy2015deep,
 author = {Andrej Karpathy and
Fei{-}Fei Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/KarpathyL15.bib},
 booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2015, Boston, MA, USA, June 7-12, 2015},
 doi = {10.1109/CVPR.2015.7298932},
 pages = {3128--3137},
 publisher = {{IEEE} Computer Society},
 timestamp = {Mon, 22 Jul 2019 01:00:00 +0200},
 title = {Deep visual-semantic alignments for generating image descriptions},
 url = {https://doi.org/10.1109/CVPR.2015.7298932},
 year = {2015}
}

@misc{khorrami_2021,
 author = {Khorrami, Khazar and R\"as\"anen, Okko},
 doi = {10.31234/osf.io/37zna},
 note = {Preprint \url{psyarxiv.com/37zn}},
 publisher = {PsyArXiv},
 title = {Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? - A computational investigation},
 url = {psyarxiv.com/37zna},
 year = {2021}
}

@inproceedings{kingma2014adam,
 author = {Diederik P. Kingma and
Jimmy Ba},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Adam: {A} Method for Stochastic Optimization},
 url = {http://arxiv.org/abs/1412.6980},
 year = {2015}
}

@article{kriegeskorte2008representational,
 author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A},
 journal = {Frontiers in systems neuroscience},
 pages = {4},
 publisher = {Frontiers},
 title = {Representational similarity analysis-connecting the branches of systems neuroscience},
 volume = {2},
 year = {2008}
}

@inproceedings{krishna2017dense,
 author = {Ranjay Krishna and
Kenji Hata and
Frederic Ren and
Li Fei{-}Fei and
Juan Carlos Niebles},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/KrishnaHRFN17.bib},
 booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
Italy, October 22-29, 2017},
 doi = {10.1109/ICCV.2017.83},
 pages = {706--715},
 publisher = {{IEEE} Computer Society},
 timestamp = {Mon, 22 Jul 2019 01:00:00 +0200},
 title = {Dense-Captioning Events in Videos},
 url = {https://doi.org/10.1109/ICCV.2017.83},
 year = {2017}
}

@inproceedings{krug2018neuron,
 author = {Krug, Andreas and Knaebel, Ren{\'e} and Stober, Sebastian},
 booktitle = {NeurIPS Workshop on Interpretability and Robustness in Audio, Speech, and Language (IRASL)},
 title = {Neuron activation profiles for interpreting convolutional speech recognition models},
 year = {2018}
}

@article{kuznetsova2020open,
 author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
 journal = {International Journal of Computer Vision},
 pages = {1--26},
 publisher = {Springer},
 title = {The open images dataset v4},
 year = {2020}
}

@inproceedings{leidal2017learning,
 author = {Leidal, Kenneth and Harwath, David and Glass, James},
 booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 organization = {IEEE},
 pages = {424--429},
 title = {Learning modality-invariant representations for speech and images},
 year = {2017}
}

@phdthesis{leidal2018neural,
 author = {Leidal, Kenneth Kenneth Knute},
 school = {Massachusetts Institute of Technology},
 title = {Neural techniques for modeling visually grounded speech},
 year = {2018}
}

@inproceedings{lin2014microsoft,
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 booktitle = {European conference on computer vision},
 organization = {Springer},
 pages = {740--755},
 title = {Microsoft {COCO}: Common objects in context},
 year = {2014}
}

@inproceedings{mangin2013learning,
 author = {Mangin, Olivier and Oudeyer, Pierre-Yves},
 booktitle = {2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)},
 organization = {IEEE},
 pages = {1--7},
 title = {Learning semantic components from subsymbolic multimodal perception},
 year = {2013}
}

@article{mangin2015mca,
 author = {Mangin, Olivier and Filliat, David and Ten Bosch, Louis and Oudeyer, Pierre-Yves},
 journal = {Plos One},
 number = {10},
 pages = {e0140732},
 publisher = {Public Library of Science},
 title = {MCA-NMF: Multimodal concept acquisition with non-negative matrix factorization},
 volume = {10},
 year = {2015}
}

@inproceedings{marelli2014sick,
 address = {Reykjavik, Iceland},
 author = {Marelli, Marco  and
Menini, Stefano  and
Baroni, Marco  and
Bentivogli, Luisa  and
Bernardi, Raffaella  and
Zamparelli, Roberto},
 booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
 pages = {216--223},
 publisher = {European Language Resources Association (ELRA)},
 title = {A {SICK} cure for the evaluation of compositional distributional semantic models},
 url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},
 year = {2014}
}

@inproceedings{Merkx2019,
 author = {Danny Merkx and Stefan L. Frank and Mirjam Ernestus},
 booktitle = {Proc. Interspeech 2019},
 doi = {10.21437/Interspeech.2019-3067},
 pages = {1841--1845},
 title = {{Language Learning Using Speech to Image Retrieval}},
 url = {http://dx.doi.org/10.21437/Interspeech.2019-3067},
 year = {2019}
}

@article{merkx2019learning,
 author = {Merkx, Danny and Frank, Stefan L},
 journal = {Natural Language Engineering},
 number = {4},
 pages = {451--466},
 publisher = {Cambridge University Press},
 title = {Learning semantic sentence representations from visually grounded language without lexical knowledge},
 volume = {25},
 year = {2019}
}

@inproceedings{miech2019howto100m,
 author = {Antoine Miech and
Dimitri Zhukov and
Jean{-}Baptiste Alayrac and
Makarand Tapaswi and
Ivan Laptev and
Josef Sivic},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/MiechZATLS19.bib},
 booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
2019, Seoul, Korea (South), October 27 - November 2, 2019},
 doi = {10.1109/ICCV.2019.00272},
 pages = {2630--2640},
 publisher = {{IEEE}},
 timestamp = {Thu, 05 Mar 2020 00:00:00 +0100},
 title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million
Narrated Video Clips},
 url = {https://doi.org/10.1109/ICCV.2019.00272},
 year = {2019}
}

@inproceedings{miech2020end,
 author = {Antoine Miech and
Jean{-}Baptiste Alayrac and
Lucas Smaira and
Ivan Laptev and
Josef Sivic and
Andrew Zisserman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/MiechASLSZ20.bib},
 booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
 doi = {10.1109/CVPR42600.2020.00990},
 pages = {9876--9886},
 publisher = {{IEEE}},
 timestamp = {Tue, 11 Aug 2020 01:00:00 +0200},
 title = {End-to-End Learning of Visual Representations From Uncurated Instructional
Videos},
 url = {https://doi.org/10.1109/CVPR42600.2020.00990},
 year = {2020}
}

@inproceedings{mikolov2013efficient,
 author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 booktitle = {1st International Conference on Learning Representations (ICLR 2013)},
 title = {Efficient Estimation of Word Representations in Vector Space},
 year = {2013}
}

@inproceedings{monfort2021spokenmoments,
 author = {Mathew Monfort and SouYoung Jin and Alexander Liu and David Harwath and Rogerio Feris and James Glass and Aude Oliva},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 title = {Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},
 year = {2021}
}

@article{monfortmoments,
 author = {Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfruend, Dan and Vondrick, Carl and others},
 doi = {10.1109/TPAMI.2019.2901464},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 numpages = {8},
 pages = {1--8},
 title = {Moments in Time Dataset: one million videos for event understanding},
 year = {2019}
}

@inproceedings{mukherjee2003visual,
 author = {Mukherjee, Niloy and Roy, Deb},
 booktitle = {Eighth European Conference on Speech Communication and Technology},
 title = {A visual context-aware multimodal system for spoken language processing},
 year = {2003}
}

@inproceedings{ngiam2011multimodal,
 author = {Jiquan Ngiam and
Aditya Khosla and
Mingyu Kim and
Juhan Nam and
Honglak Lee and
Andrew Y. Ng},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/NgiamKKNLN11.bib},
 booktitle = {Proceedings of the 28th International Conference on Machine Learning,
{ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
 editor = {Lise Getoor and
Tobias Scheffer},
 pages = {689--696},
 publisher = {Omnipress},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {Multimodal Deep Learning},
 url = {https://icml.cc/2011/papers/399\_icmlpaper.pdf},
 year = {2011}
}

@misc{nortje2020direct,
 archiveprefix = {arXiv},
 author = {Leanne Nortje and Herman Kamper},
 eprint = {2012.05680},
 note = {ArXiv preprint: \url{https://arxiv.org/abs/2012.05680}},
 primaryclass = {cs.CL},
 title = {Direct multimodal few-shot learning of speech and images},
 year = {2020}
}

@inproceedings{nortje2020unsupervised,
 author = {Nortje, Leanne and Kamper, Herman},
 booktitle = {Interspeech},
 pages = {2712--2716},
 title = {Unsupervised vs. Transfer Learning for Multimodal One-Shot Matching of Speech and Images},
 year = {2020}
}

@inproceedings{ohishi2020pair,
 author = {Ohishi, Yasunori and Kimura, Akisato and Kawanishi, Takahito and Kashino, Kunio and Harwath, David and Glass, James},
 booktitle = {Proc. Interspeech 2020},
 pages = {1486--1490},
 title = {Pair Expansion for Learning Multilingual Semantic Embeddings using Disjoint Visually-grounded Speech Audio Datasets},
 year = {2020}
}

@inproceedings{ohishi2020trilingual,
 author = {Yasunori Ohishi and
Akisato Kimura and
Takahito Kawanishi and
Kunio Kashino and
David Harwath and
James R. Glass},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/OhishiKKKHG20.bib},
 booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
 doi = {10.1109/ICASSP40776.2020.9053428},
 pages = {4352--4356},
 publisher = {{IEEE}},
 timestamp = {Thu, 23 Jul 2020 01:00:00 +0200},
 title = {Trilingual Semantic Embeddings of Visually Grounded Speech with Self-Attention
Mechanisms},
 url = {https://doi.org/10.1109/ICASSP40776.2020.9053428},
 year = {2020}
}

@misc{olaleye2020localisation,
 archiveprefix = {arXiv},
 author = {Kayode Olaleye and Benjamin van Niekerk and Herman Kamper},
 eprint = {2012.07396},
 note = {ArXiv preprint: \url{https://arxiv.org/abs/2012.07396}},
 primaryclass = {cs.CL},
 title = {Towards localisation of keywords in speech using weak supervision},
 year = {2020}
}

@inproceedings{owens2016ambient,
 author = {Owens, Andrew and Wu, Jiajun and McDermott, Josh H and Freeman, William T and Torralba, Antonio},
 booktitle = {European conference on computer vision},
 organization = {Springer},
 pages = {801--816},
 title = {Ambient sound provides supervision for visual learning},
 year = {2016}
}

@inproceedings{owens2016visually,
 author = {Andrew Owens and
Phillip Isola and
Josh H. McDermott and
Antonio Torralba and
Edward H. Adelson and
William T. Freeman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/OwensIMTAF16.bib},
 booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
 doi = {10.1109/CVPR.2016.264},
 pages = {2405--2413},
 publisher = {{IEEE} Computer Society},
 timestamp = {Wed, 12 Jul 2017 01:00:00 +0200},
 title = {Visually Indicated Sounds},
 url = {https://doi.org/10.1109/CVPR.2016.264},
 year = {2016}
}

@inproceedings{Pasad2019,
 author = {Ankita Pasad and Bowen Shi and Herman Kamper and Karen Livescu},
 booktitle = {Proc. Interspeech 2019},
 doi = {10.21437/Interspeech.2019-3051},
 pages = {4195--4199},
 title = {{On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval}},
 url = {http://dx.doi.org/10.21437/Interspeech.2019-3051},
 year = {2019}
}

@inproceedings{paul1992design,
 author = {Paul, Douglas B.  and
Baker, Janet M.},
 booktitle = {Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992},
 title = {The Design for the {W}all {S}treet {J}ournal-based {CSR} Corpus},
 url = {https://www.aclweb.org/anthology/H92-1073},
 year = {1992}
}

@inproceedings{PontTuset_eccv2020,
 author = {Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu Soricut and Vittorio Ferrari},
 booktitle = {ECCV},
 title = {Connecting Vision and Language with Localized Narratives},
 year = {2020}
}

@inproceedings{Povey_ASRU2011,
 author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
 booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
 keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
 location = {Hilton Waikoloa Village, Big Island, Hawaii, US},
 note = {IEEE Catalog No.: CFP11SRW-USB},
 publisher = {IEEE Signal Processing Society},
 title = {The Kaldi Speech Recognition Toolkit},
 year = {2011}
}

@inproceedings{rosenberg_v-measure_2007,
 address = {Prague, Czech Republic},
 author = {Rosenberg, Andrew  and
Hirschberg, Julia},
 booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})},
 pages = {410--420},
 publisher = {Association for Computational Linguistics},
 title = {{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure},
 url = {https://www.aclweb.org/anthology/D07-1043},
 year = {2007}
}

@misc{rouditchenko2020avlnet,
 author = {Rouditchenko, Andrew and Boggust, Angie and Harwath, David and Joshi, Dhiraj and Thomas, Samuel and Audhkhasi, Kartik and Feris, Rogerio and Kingsbury, Brian and Picheny, Michael and Torralba, Antonio and others},
 eprint = {2006.09199},
 title = {Avlnet: Learning audio-visual language representations from instructional videos},
 year = {2020}
}

@phdthesis{roy1999learning,
 author = {Roy, Deb},
 school = {MIT Media Laboratory},
 title = {Learning from sights and sounds: {A} computational model},
 year = {1999}
}

@inproceedings{roy2000grounded,
 author = {Roy, Deb},
 booktitle = {Sixth International Conference on Spoken Language Processing},
 title = {Grounded speech communication},
 year = {2000}
}

@article{roy2000learning,
 author = {Roy, Deb},
 journal = {Evolution of communication},
 number = {1},
 pages = {33--56},
 publisher = {John Benjamins},
 title = {Learning visually grounded words and syntax of natural spoken language},
 volume = {4},
 year = {2000}
}

@article{roy2001situation,
 author = {Roy, DK},
 journal = {PROCEEDINGS-INSTITUTE OF ACOUSTICS},
 number = {3},
 pages = {231--246},
 publisher = {INSTITUTE OF ACOUSTICS},
 title = {Situation-aware spoken language processing},
 volume = {23},
 year = {2001}
}

@techreport{roy2002grounding,
 author = {Roy, Deb and Hsiao, Kai-Yuh and Gorniak, Peter and Mukherjee, Niloy},
 institution = {AAAI, Tech. Rep},
 title = {Grounding natural spoken language semantics in visual perception and motor control},
 year = {2002}
}

@article{roy2002learning,
 author = {Roy, Deb K},
 journal = {Computer speech \& language},
 number = {3-4},
 pages = {353--385},
 publisher = {Elsevier},
 title = {Learning visually grounded words and syntax for a scene description task},
 volume = {16},
 year = {2002}
}

@inproceedings{roy2002towards,
 author = {Roy, Deb},
 booktitle = {Proceedings. Fourth IEEE International Conference on Multimodal Interfaces},
 organization = {IEEE},
 pages = {105--110},
 title = {Towards visually-grounded spoken language acquisition},
 year = {2002}
}

@inproceedings{roy2002trainable,
 author = {Roy, Deb and Gorniak, Peter and Mukherjee, Niloy and Juster, Josh},
 booktitle = {Seventh International Conference on Spoken Language Processing},
 title = {A trainable spoken language understanding system for visual object selection},
 year = {2002}
}

@article{roy2003grounded,
 author = {Roy, Deb},
 journal = {IEEE Transactions on Multimedia},
 number = {2},
 pages = {197--209},
 publisher = {IEEE},
 title = {Grounded spoken language acquisition: Experiments in word learning},
 volume = {5},
 year = {2003}
}

@article{roy2005grounding,
 author = {Roy, Deb},
 journal = {Trends in cognitive sciences},
 number = {8},
 pages = {389--396},
 publisher = {Elsevier},
 title = {Grounding words in perception and action: computational insights},
 volume = {9},
 year = {2005}
}

@article{roy2005towards,
 author = {Roy, Deb and Mukherjee, Niloy},
 journal = {Computer Speech \& Language},
 number = {2},
 pages = {227--248},
 publisher = {Elsevier},
 title = {Towards situated speech understanding: Visual context priming of language models},
 volume = {19},
 year = {2005}
}

@article{roypentland2002learning,
 author = {Roy, Deb K and Pentland, Alex P},
 journal = {Cognitive science},
 number = {1},
 pages = {113--146},
 publisher = {Wiley Online Library},
 title = {Learning words from sights and sounds: A computational model},
 volume = {26},
 year = {2002}
}

@misc{sanabria2021talk,
 archiveprefix = {arXiv},
 author = {Ramon Sanabria and Austin Waters and Jason Baldridge},
 eprint = {2104.01894},
 note = {Preprint: \url{https://arxiv.org/abs/2104.01894}},
 primaryclass = {cs.CL},
 title = {Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval},
 year = {2021}
}

@inproceedings{scharenborg2018linguistic,
 author = {Odette Scharenborg and
Laurent Besacier and
Alan W. Black and
Mark Hasegawa{-}Johnson and
Florian Metze and
Graham Neubig and
Sebastian St{\"{u}}ker and
Pierre Godard and
Markus M{\"{u}}ller and
Lucas Ondel and
Shruti Palaskar and
Philip Arthur and
Francesco Ciannella and
Mingxing Du and
Elin Larsen and
Danny Merkx and
Rachid Riad and
Liming Wang and
Emmanuel Dupoux},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icassp/ScharenborgBBHM18.bib},
 booktitle = {2018 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2018, Calgary, AB, Canada, April 15-20, 2018},
 doi = {10.1109/ICASSP.2018.8461761},
 pages = {4979--4983},
 publisher = {{IEEE}},
 timestamp = {Mon, 16 Sep 2019 01:00:00 +0200},
 title = {Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages:
Summary of the "Speaking Rosetta" {JSALT} 2017 Workshop},
 url = {https://doi.org/10.1109/ICASSP.2018.8461761},
 year = {2018}
}

@phdthesis{schatz2016abx,
 author = {Schatz, Thomas},
 school = {Université Paris 6 (UPMC)},
 title = {ABX-discriminability measures and applications},
 year = {2016}
}

@misc{scholten2020learning,
 author = {Scholten, Sebastiaan and Merkx, Danny and Scharenborg, Odette},
 eprint = {arXiv preprint arXiv:2006.00512},
 note = {Preprint: \url{https://arxiv.org/abs/2006.00512}},
 title = {Learning to Recognise Words using Visually Grounded Speech},
 year = {2020}
}

@misc{shukla2020learning,
 archiveprefix = {arXiv},
 author = {Abhinav Shukla and Stavros Petridis and Maja Pantic},
 eprint = {2007.04134},
 note = {Preprint: \url{https://arxiv.org/abs/2007.04134}},
 primaryclass = {eess.AS},
 title = {Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision},
 year = {2020}
}

@inproceedings{simonyan2015deep,
 author = {Karen Simonyan and
Andrew Zisserman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 url = {http://arxiv.org/abs/1409.1556},
 year = {2015}
}

@inproceedings{smith2017cyclical,
 author = {Smith, Leslie N},
 booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 organization = {IEEE},
 pages = {464--472},
 title = {Cyclical learning rates for training neural networks},
 year = {2017}
}

@misc{speech_coco,
 author = {William N. Havard and Laurent Besacier},
 doi = {10.5281/zenodo.4282267},
 note = {\url{https://doi.org/10.5281/zenodo.4282267}},
 publisher = {Zenodo},
 title = {{SPEECH-COCO}},
 year = {2017}
}

@article{sun2019learning,
 author = {Sun, Chen and Baradel, Fabien and Murphy, Kevin and Schmid, Cordelia},
 journal = {arXiv preprint arXiv:1906.05743},
 title = {Learning video representations using contrastive bidirectional transformer},
 year = {2019}
}

@inproceedings{sun2019videobert,
 author = {Chen Sun and
Austin Myers and
Carl Vondrick and
Kevin Murphy and
Cordelia Schmid},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/SunMV0S19.bib},
 booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
2019, Seoul, Korea (South), October 27 - November 2, 2019},
 doi = {10.1109/ICCV.2019.00756},
 pages = {7463--7472},
 publisher = {{IEEE}},
 timestamp = {Wed, 21 Oct 2020 01:00:00 +0200},
 title = {VideoBERT: {A} Joint Model for Video and Language Representation Learning},
 url = {https://doi.org/10.1109/ICCV.2019.00756},
 year = {2019}
}

@inproceedings{synnaeve2014learning,
 author = {Synnaeve, Gabriel and Versteegh, Maarten and Dupoux, Emmanuel},
 booktitle = {NIPS Workshop on Learning Semantics},
 title = {Learning words from images and speech},
 year = {2014}
}

@misc{synthetically_2017,
 author = {Grzegorz Chrupa{\l}a and Lieke Gelderloos and Afra Alishahi},
 doi = {10.5281/zenodo.400926},
 note = {\url{https://doi.org/10.5281/zenodo.794832}},
 publisher = {Zenodo},
 title = {Synthetically Spoken {COCO}},
 year = {2017}
}

@article{tjandra_transformer_2020,
 abstract = {In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019. The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels. In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) re-synthesize the audio from novel speakers. The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate. Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformer-based inverter for the speech synthesis given the extracted codebook. Additionally, we also explored several regularization methods to improve performance even further.},
 author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
 file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/VDQIHUZ6/Tjandra et al. - 2020 - Transformer VQ-VAE for Unsupervised Unit Discovery.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/BN4F4W78/2005.html:text/html},
 journal = {arXiv:2005.11676 [cs, eess]},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2020},
 note = {arXiv: 2005.11676},
 shorttitle = {Transformer {VQ}-{VAE} for {Unsupervised} {Unit} {Discovery} and {Speech} {Synthesis}},
 title = {Transformer {VQ}-{VAE} for {Unsupervised} {Unit} {Discovery} and {Speech} {Synthesis}: {ZeroSpeech} 2020 {Challenge}},
 url = {http://arxiv.org/abs/2005.11676},
 urldate = {2020-06-24},
 year = {2020}
}

@article{tjandra_vqvae_2019,
 abstract = {We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.},
 author = {Tjandra, Andros and Sisman, Berrak and Zhang, Mingyang and Sakti, Sakriani and Li, Haizhou and Nakamura, Satoshi},
 file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/VS6MSHZG/Tjandra et al. - 2019 - VQVAE Unsupervised Unit Discovery and Multi-scale .pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/QHPS2B9V/1905.html:text/html},
 journal = {arXiv:1905.11449 [cs, eess]},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2019},
 note = {arXiv: 1905.11449},
 title = {{VQVAE} {Unsupervised} {Unit} {Discovery} and {Multi}-scale {Code2Spec} {Inverter} for {Zerospeech} {Challenge} 2019},
 url = {http://arxiv.org/abs/1905.11449},
 urldate = {2020-06-24},
 year = {2019}
}

@inproceedings{van2017neural,
 author = {A{\"{a}}ron van den Oord and
Oriol Vinyals and
Koray Kavukcuoglu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/OordVK17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {6306--6315},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html},
 year = {2017}
}

@mastersthesis{van2018encoding,
 address = {the Netherlands},
 author = {van der Laan, Mark},
 school = {Tilburg University},
 title = {Encoding of speaker identity in a Neural Network model of Visually Grounded Speech perception},
 year = {2018}
}

@misc{van2020vector,
 author = {van Niekerk, Benjamin and Nortje, Leanne and Kamper, Herman},
 note = {Preprint: \url{https://arxiv.org/abs/2005.09409}},
 title = {Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge},
 year = {2020}
}

@inproceedings{vanoord2017neural,
 author = {A{\"{a}}ron van den Oord and
Oriol Vinyals and
Koray Kavukcuoglu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/OordVK17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {6306--6315},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html},
 year = {2017}
}

@inproceedings{vasudevan2018object,
 author = {Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
 booktitle = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 organization = {IEEE},
 pages = {1861--1870},
 title = {Object referring in visual scene with spoken language},
 year = {2018}
}

@inproceedings{voita2020informationtheoretic,
 address = {Online},
 author = {Voita, Elena  and
Titov, Ivan},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.14},
 pages = {183--196},
 publisher = {Association for Computational Linguistics},
 title = {Information-Theoretic Probing with Minimum Description Length},
 url = {https://www.aclweb.org/anthology/2020.emnlp-main.14},
 year = {2020}
}

@misc{wang2020align,
 author = {Wang, Liming and Wang, Xinsheng and Hasegawa-Johnson, Mark and Scharenborg, Odette and Dehak, Najim},
 title = {Align or attend? Toward more efficient and accurate spoken word discovery using speech-to-image retrieval},
 url = {https://www.researchgate.net/publication/345007040_ALIGN_OR_ATTEND_TOWARD_MORE_EFFICIENT_AND_ACCURATE_SPOKEN_WORD_DISCOVERY_USING_SPEECH-TO-IMAGE_RETRIEVAL},
 year = {2020}
}

@inproceedings{wang2020dnn,
 author = {Wang, Liming and Hasegawa-Johnson, Mark},
 booktitle = {Interspeech},
 pages = {1456--1460},
 title = {A {DNN-HMM-DNN} Hybrid Model for Discovering Word-like Units from Spoken Captions and Image Regions},
 year = {2020}
}

@misc{wang2020show,
 author = {Wang, Xinsheng and Feng, Siyuan and Zhu, Jihua and Hasegawa-Johnson, Mark and Scharenborg, Odette},
 eprint = {2010.12267},
 title = {Show and Speak: Directly Synthesize Spoken Description of Images},
 year = {2020}
}

@misc{william_n_havard_2018_1495070,
 author = {William N. Havard and
Jean-Pierre Chevrot and
Laurent Besacier},
 doi = {10.5281/zenodo.1495070},
 note = {\url{https://doi.org/10.5281/zenodo.1495070}},
 publisher = {Zenodo},
 title = {Synthetically Spoken {STAIR}},
 url = {https://doi.org/10.5281/zenodo.1495070},
 year = {2018}
}

@article{yu2004multimodal,
 author = {Yu, Chen and Ballard, Dana H},
 journal = {ACM Transactions on Applied Perception (TAP)},
 number = {1},
 pages = {57--80},
 publisher = {ACM New York, NY, USA},
 title = {A multimodal learning interface for grounding spoken language in sensory perceptions},
 volume = {1},
 year = {2004}
}

@article{yu2005role,
 author = {Yu, Chen and Ballard, Dana H and Aslin, Richard N},
 journal = {Cognitive science},
 number = {6},
 pages = {961--1005},
 publisher = {Wiley Online Library},
 title = {The role of embodied intention in early lexical acquisition},
 volume = {29},
 year = {2005}
}

@article{zhang1996birch,
 author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
 journal = {ACM sigmod record},
 number = {2},
 pages = {103--114},
 publisher = {ACM New York, NY, USA},
 title = {BIRCH: an efficient data clustering method for very large databases},
 volume = {25},
 year = {1996}
}

@article{zhang2020sound,
 author = {Zhang, Mingxin and Tanaka, Tomohiro and Hou, Wenxin and Gao, Shengzhou and Shinozaki, Takahiro},
 journal = {Proc. Interspeech 2020},
 pages = {4183--4187},
 title = {Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition},
 year = {2020}
}

@inproceedings{zhangstudy,
 author = {Zhang, Hanjuan},
 booktitle = {Proceedings of the 3rd International Conference on Computer Engineering, Information Science and Internet Technology},
 title = {Study of Unsupervised Learning of Visual Objects with Spoken Words based on Siamese Network},
 year = {2019}
}

@inproceedings{zhou2014learning,
 author = {Bolei Zhou and
{\`{A}}gata Lapedriza and
Jianxiong Xiao and
Antonio Torralba and
Aude Oliva},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZhouLXTO14.bib},
 booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada},
 editor = {Zoubin Ghahramani and
Max Welling and
Corinna Cortes and
Neil D. Lawrence and
Kilian Q. Weinberger},
 pages = {487--495},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Learning Deep Features for Scene Recognition using Places Database},
 url = {https://proceedings.neurips.cc/paper/2014/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html},
 year = {2014}
}

@inproceedings{zhou2018end,
 author = {Luowei Zhou and
Yingbo Zhou and
Jason J. Corso and
Richard Socher and
Caiming Xiong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/ZhouZCSX18.bib},
 booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
 doi = {10.1109/CVPR.2018.00911},
 pages = {8739--8748},
 publisher = {{IEEE} Computer Society},
 timestamp = {Wed, 06 Feb 2019 00:00:00 +0100},
 title = {End-to-End Dense Video Captioning With Masked Transformer},
 url = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Zhou\_End-to-End\_Dense\_Video\_CVPR\_2018\_paper.html},
 year = {2018}
}

@article{zhou2019semantic,
 author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
 journal = {International Journal of Computer Vision},
 number = {3},
 pages = {302--321},
 publisher = {Kluwer Academic Publishers Norwell, MA, USA},
 title = {Semantic Understanding of Scenes Through the ADE20K Dataset},
 volume = {127},
 year = {2019}
}

@inproceedings{zilly2017recurrent,
 author = {Julian Georg Zilly and
Rupesh Kumar Srivastava and
Jan Koutn{\'{\i}}k and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/ZillySKS17.bib},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning,
{ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
 editor = {Doina Precup and
Yee Whye Teh},
 pages = {4189--4198},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {Recurrent Highway Networks},
 url = {http://proceedings.mlr.press/v70/zilly17a.html},
 volume = {70},
 year = {2017}
}

