@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={706--715},
  year={2017}
}

@inproceedings{zhou2018end,
  title={End-to-end dense video captioning with masked transformer},
  author={Zhou, Luowei and Zhou, Yingbo and Corso, Jason J and Socher, Richard and Xiong, Caiming},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8739--8748},
  year={2018}
}


@inproceedings{owens2016ambient,
  title={Ambient sound provides supervision for visual learning},
  author={Owens, Andrew and Wu, Jiajun and McDermott, Josh H and Freeman, William T and Torralba, Antonio},
  booktitle={European conference on computer vision},
  pages={801--816},
  year={2016},
  organization={Springer}
}

@inproceedings{owens2016visually,
  title={Visually indicated sounds},
  author={Owens, Andrew and Isola, Phillip and McDermott, Josh and Torralba, Antonio and Adelson, Edward H and Freeman, William T},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2405--2413},
  year={2016}
}

@article{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={892--900},
  year={2016}
}

@article{bernardi2016automatic,
  title={Automatic description generation from images: A survey of models, datasets, and evaluation measures},
  author={Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
  journal={Journal of Artificial Intelligence Research},
  volume={55},
  pages={409--442},
  year={2016}
}

@InProceedings{monfort2021spokenmoments,
    title={Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions},
    author={Mathew Monfort and SouYoung Jin and Alexander Liu and David Harwath and Rogerio Feris and James Glass and Aude Oliva},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month={June},
    year={2021}
}


@article{monfortmoments,
  title={Moments in Time Dataset: one million videos for event understanding},
  author={Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfruend, Dan and Vondrick, Carl and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2019},
  issn={0162-8828},
  pages={1--8},
  numpages={8},
  doi={10.1109/TPAMI.2019.2901464},
}

@misc{william_n_havard_2018_1495070,
  author       = {William N. Havard and
                  Jean-Pierre Chevrot and
                  Laurent Besacier},
  title        = {Synthetically Spoken {STAIR}},
  month        = nov,
  year         = 2018,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.1495070},
  url          = {https://doi.org/10.5281/zenodo.1495070},
  note         = {\url{https://doi.org/10.5281/zenodo.1495070}}
}
@inproceedings{ohishi2020pair,
  title={Pair Expansion for Learning Multilingual Semantic Embeddings using Disjoint Visually-grounded Speech Audio Datasets},
  author={Ohishi, Yasunori and Kimura, Akisato and Kawanishi, Takahito and Kashino, Kunio and Harwath, David and Glass, James},
  booktitle={Proc. Interspeech 2020},
  pages={1486--1490},
  year={2020}
}


@article{kuznetsova2020open,
  title={The open images dataset v4},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal={International Journal of Computer Vision},
  pages={1--26},
  year={2020},
  publisher={Springer}
}

@article{zhou2019semantic,
  title={Semantic Understanding of Scenes Through the ADE20K Dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  journal={International Journal of Computer Vision},
  volume={127},
  number={3},
  pages={302--321},
  year={2019},
  publisher={Kluwer Academic Publishers Norwell, MA, USA}
}

@inproceedings{PontTuset_eccv2020,
  author    = {Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu Soricut and Vittorio Ferrari},
  title     = {Connecting Vision and Language with Localized Narratives},
  booktitle = {ECCV},
  year      = {2020}
}

@misc{sanabria2021talk,
      title={Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval}, 
      author={Ramon Sanabria and Austin Waters and Jason Baldridge},
      year={2021},
      eprint={2104.01894},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note={Preprint: \url{https://arxiv.org/abs/2104.01894}}
}
@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2630--2640},
  year={2019}
}

@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6309--6318},
  year={2017}
}

@inproceedings{baker2010lucid,
  title={LUCID: a corpus of spontaneous and read clear speech in British English},
  author={Baker, Rachel and Hazan, Valerie},
  booktitle={DiSS-LPSS Joint Workshop 2010},
  year={2010}
}

@inproceedings{synnaeve2014learning,
  title={Learning words from images and speech},
  author={Synnaeve, Gabriel and Versteegh, Maarten and Dupoux, Emmanuel},
  year={2014},
  booktitle={NIPS Workshop on Learning Semantics}
}

@inproceedings{Pasad2019,
  author={Ankita Pasad and Bowen Shi and Herman Kamper and Karen Livescu},
  title={{On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={4195--4199},
  doi={10.21437/Interspeech.2019-3051},
  url={http://dx.doi.org/10.21437/Interspeech.2019-3051}
}
@article{zhang1996birch,
  title={BIRCH: an efficient data clustering method for very large databases},
  author={Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
  journal={ACM sigmod record},
  volume={25},
  number={2},
  pages={103--114},
  year={1996},
  publisher={ACM New York, NY, USA}
}

@inproceedings{belinkov2017analyzing,
  title={Analyzing hidden representations in end-to-end automatic speech recognition systems},
  author={Belinkov, Yonatan and Glass, James},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={2438--2448},
  year={2017}
}


@article{Belinkov_2019,
   title={Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition},
   url={http://dx.doi.org/10.21437/interspeech.2019-2599},
   DOI={10.21437/interspeech.2019-2599},
   journal={Interspeech 2019},
   publisher={ISCA},
   author={Belinkov, Yonatan and Ali, Ahmed and Glass, James},
   year={2019},
   month={Sep}
}


@inproceedings{krug2018neuron,
  title={Neuron activation profiles for interpreting convolutional speech recognition models},
  author={Krug, Andreas and Knaebel, Ren{\'e} and Stober, Sebastian},
  year={2018},
  booktitle={NeurIPS Workshop on Interpretability and Robustness in Audio, Speech, and Language (IRASL)}
}

@inproceedings{zilly2017recurrent,
  title={Recurrent highway networks},
  author={Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\i}k, Jan and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={4189--4198},
  year={2017},
  organization={PMLR}
}
@inproceedings{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle={1st International Conference on Learning Representations (ICLR 2013)},
  year={2013}
}

@misc{khorrami_2021,
 title={Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? - A computational investigation},
 url={psyarxiv.com/37zna},
 DOI={10.31234/osf.io/37zna},
 publisher={PsyArXiv},
 author={Khorrami, Khazar and R\"as\"anen, Okko},
 year={2021},
 note={Preprint \url{psyarxiv.com/37zn}},
 month={Feb}
}
@unpublished{              
anonymous2021discrete,              
title={Discrete representations in neural models of spoken language},              
author={Anonymous},              
journal={OpenReview Preprint},              
year={2021},              
note={Anonymous preprint \url{https://openreview.net/forum?id=ShVI_e0NuB4}}          
}
@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015}
}

@inproceedings{karpathy2014deep,
  title={Deep fragment embeddings for bidirectional image sentence mapping},
  author={Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
  booktitle={Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 2},
  pages={1889--1897},
  year={2014}
}

@inproceedings{paul1992design,
  title={The design for the Wall Street Journal-based CSR corpus},
  author={Paul, Douglas B and Baker, Janet},
  booktitle={Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992},
  year={1992}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@misc{chung2014empirical,
      title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
      author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1412.3555},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      note={Preprint: \url{https://arxiv.org/abs/1409.1556}},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{zhou2014learning,
  title={Learning deep features for scene recognition using places database},
  author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  year={2014},
  booktitle={Advances in Neural Information Processing Systems 27 (NIPS)},
  publisher={Neural Information Processing Systems Foundation}
}

@misc{synthetically_2017,
title={Synthetically Spoken {COCO}},
DOI={10.5281/zenodo.400926},
note={\url{https://doi.org/10.5281/zenodo.794832}},
publisher={Zenodo},
author={Grzegorz Chrupa{\l}a and Lieke Gelderloos and Afra Alishahi},
year={2017},
month={Apr}}

@misc{speech_coco,
title={{SPEECH-COCO}},
DOI={10.5281/zenodo.4282267},
note={\url{https://doi.org/10.5281/zenodo.4282267}},
publisher={Zenodo},
author={William N. Havard and Laurent Besacier},
year={2017},
month={Jun}}



@inproceedings{lin2014microsoft,
  title={Microsoft {COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@MISC{1993STIN...9327403G,
       author = {{Garofolo}, J.~S. and {Lamel}, L.~F. and {Fisher}, W.~M. and {Fiscus}, J.~G. and {Pallett}, D.~S.},
        title = "{DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1}",
     keywords = {Cd-Rom, Data Bases, Linguistics, Phonemes, Phonetics, Speech, English Language, Speech Recognition, Verbal Communication, Words (Language), Communications and Radar},
 howpublished = {NASA STI/Recon Technical Report N},
         year = 1993,
        month = feb,
        pages = {27403},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1993STIN...9327403G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@misc{shukla2020learning,
      title={Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision}, 
      author={Abhinav Shukla and Stavros Petridis and Maja Pantic},
      year={2020},
      eprint={2007.04134},
      note={Preprint: \url{https://arxiv.org/abs/2007.04134}},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{olaleye2020localisation,
      title={Towards localisation of keywords in speech using weak supervision}, 
      author={Kayode Olaleye and Benjamin van Niekerk and Herman Kamper},
      year={2020},
      eprint={2012.07396},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note={ArXiv preprint: \url{https://arxiv.org/abs/2012.07396}}
}

@misc{nortje2020direct,
      title={Direct multimodal few-shot learning of speech and images}, 
      author={Leanne Nortje and Herman Kamper},
      year={2020},
      eprint={2012.05680},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note={ArXiv preprint: \url{https://arxiv.org/abs/2012.05680}}
}


@misc{hsu2020textfree,
      title={Text-Free Image-to-Speech Synthesis Using Learned Segmental Units}, 
      author={Wei-Ning Hsu and David Harwath and Christopher Song and James Glass},
      year={2020},
      eprint={2012.15454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note={Preprint: \url{https://arxiv.org/abs/2012.15454}}
}

@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={ICML},
  year={2011}
}


@phdthesis{roy1999learning,
  title={Learning from sights and sounds: {A} computational model},
  author={Roy, Deb},
  school={MIT Media Laboratory},
  year={1999}
}


@inproceedings{mangin2013learning,
  title={Learning semantic components from subsymbolic multimodal perception},
  author={Mangin, Olivier and Oudeyer, Pierre-Yves},
  booktitle={2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)},
  pages={1--7},
  year={2013},
  organization={IEEE}
}


@article{mangin2015mca,
  title={MCA-NMF: Multimodal concept acquisition with non-negative matrix factorization},
  author={Mangin, Olivier and Filliat, David and Ten Bosch, Louis and Oudeyer, Pierre-Yves},
  journal={Plos One},
  volume={10},
  number={10},
  pages={e0140732},
  year={2015},
  publisher={Public Library of Science}
}


@article{yu2005role,
  title={The role of embodied intention in early lexical acquisition},
  author={Yu, Chen and Ballard, Dana H and Aslin, Richard N},
  journal={Cognitive science},
  volume={29},
  number={6},
  pages={961--1005},
  year={2005},
  publisher={Wiley Online Library}
}


@article{roypentland2002learning,
  title={Learning words from sights and sounds: A computational model},
  author={Roy, Deb K and Pentland, Alex P},
  journal={Cognitive science},
  volume={26},
  number={1},
  pages={113--146},
  year={2002},
  publisher={Wiley Online Library}
}

@inproceedings{ebert2020visuospatial,
    title = "A Visuospatial Dataset for Naturalistic Verb Learning",
    author = "Ebert, Dylan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.starsem-1.16",
    pages = "143--153",
}

@misc{wang2020align,
  title={Align or attend? Toward more efficient and accurate spoken word discovery using speech-to-image retrieval},
  author={Wang, Liming and Wang, Xinsheng and Hasegawa-Johnson, Mark and Scharenborg, Odette and Dehak, Najim},
  year={2020},
  url={https://www.researchgate.net/publication/345007040_ALIGN_OR_ATTEND_TOWARD_MORE_EFFICIENT_AND_ACCURATE_SPOKEN_WORD_DISCOVERY_USING_SPEECH-TO-IMAGE_RETRIEVAL}
}


@inproceedings{wang2020dnn,
  title={A {DNN-HMM-DNN} Hybrid Model for Discovering Word-like Units from Spoken Captions and Image Regions},
  author={Wang, Liming and Hasegawa-Johnson, Mark},
  booktitle={Interspeech},
  pages={1456--1460},
  year={2020}
}


@inproceedings{zhangstudy,
  title={Study of Unsupervised Learning of Visual Objects with Spoken Words based on Siamese Network},
  author={Zhang, Hanjuan},
  year={2019},
  booktitle={Proceedings of the 3rd International Conference on Computer Engineering, Information Science and Internet Technology}
}


@article{roy2001situation,
  title={Situation-aware spoken language processing},
  author={Roy, DK},
  journal={PROCEEDINGS-INSTITUTE OF ACOUSTICS},
  volume={23},
  number={3},
  pages={231--246},
  year={2001},
  publisher={INSTITUTE OF ACOUSTICS}
}

@inproceedings{roy2000grounded,
  title={Grounded speech communication},
  author={Roy, Deb},
  booktitle={Sixth International Conference on Spoken Language Processing},
  year={2000}
}


@techreport{roy2002grounding,
  title={Grounding natural spoken language semantics in visual perception and motor control},
  author={Roy, Deb and Hsiao, Kai-Yuh and Gorniak, Peter and Mukherjee, Niloy},
  year={2002},
  institution={AAAI, Tech. Rep}
}


@article{zhang2020sound,
  title={Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition},
  author={Zhang, Mingxin and Tanaka, Tomohiro and Hou, Wenxin and Gao, Shengzhou and Shinozaki, Takahiro},
  journal={Proc. Interspeech 2020},
  pages={4183--4187},
  year={2020}
}


@misc{rouditchenko2020avlnet,
  title={Avlnet: Learning audio-visual language representations from instructional videos},
  author={Rouditchenko, Andrew and Boggust, Angie and Harwath, David and Joshi, Dhiraj and Thomas, Samuel and Audhkhasi, Kartik and Feris, Rogerio and Kingsbury, Brian and Picheny, Michael and Torralba, Antonio and others},
  eprint={2006.09199},
  year={2020}
}


@inproceedings{boggust2019grounding,
  title={Grounding Spoken Words in Unlabeled Video.},
  author={Boggust, Angie W and Audhkhasi, Kartik and Joshi, Dhiraj and Harwath, David and Thomas, Samuel and Feris, Rog{\'e}rio Schmidt and Gutfreund, Danny and Zhang, Yang and Torralba, Antonio and Picheny, Michael and others},
  booktitle={CVPR Workshops},
  pages={29--32},
  year={2019}
}


@mastersthesis{van2018encoding,
  title={Encoding of speaker identity in a Neural Network model of Visually Grounded Speech perception},
  author={van der Laan, Mark},
  year={2018},
  school={Tilburg University},
  address={the Netherlands}
}


@inproceedings{hsu2019transfer,
  author={Wei-Ning Hsu and David Harwath and James Glass},
  title={{Transfer Learning from Audio-Visual Grounding to Speech Recognition}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={3242--3246},
  doi={10.21437/Interspeech.2019-1227},
  url={http://dx.doi.org/10.21437/Interspeech.2019-1227}
}

@phdthesis{leidal2018neural,
  title={Neural techniques for modeling visually grounded speech},
  author={Leidal, Kenneth Kenneth Knute},
  year={2018},
  school={Massachusetts Institute of Technology}
}



@article{yu2004multimodal,
  title={A multimodal learning interface for grounding spoken language in sensory perceptions},
  author={Yu, Chen and Ballard, Dana H},
  journal={ACM Transactions on Applied Perception (TAP)},
  volume={1},
  number={1},
  pages={57--80},
  year={2004},
  publisher={ACM New York, NY, USA}
}


@article{roy2005grounding,
  title={Grounding words in perception and action: computational insights},
  author={Roy, Deb},
  journal={Trends in cognitive sciences},
  volume={9},
  number={8},
  pages={389--396},
  year={2005},
  publisher={Elsevier}
}


@inproceedings{vasudevan2018object,
  title={Object referring in visual scene with spoken language},
  author={Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1861--1870},
  year={2018},
  organization={IEEE}
}


@inproceedings{roy2002towards,
  title={Towards visually-grounded spoken language acquisition},
  author={Roy, Deb},
  booktitle={Proceedings. Fourth IEEE International Conference on Multimodal Interfaces},
  pages={105--110},
  year={2002},
  organization={IEEE}
}


@article{roy2003grounded,
  title={Grounded spoken language acquisition: Experiments in word learning},
  author={Roy, Deb},
  journal={IEEE Transactions on Multimedia},
  volume={5},
  number={2},
  pages={197--209},
  year={2003},
  publisher={IEEE}
}


@inproceedings{gorniak2003visually,
  title={A visually grounded natural language interface for reference to spatial scenes},
  author={Gorniak, Peter and Roy, Deb},
  booktitle={Proceedings of the 5th international conference on Multimodal interfaces},
  pages={219--226},
  year={2003}
}


@inproceedings{mukherjee2003visual,
  title={A visual context-aware multimodal system for spoken language processing},
  author={Mukherjee, Niloy and Roy, Deb},
  booktitle={Eighth European Conference on Speech Communication and Technology},
  year={2003}
}


@inproceedings{ohishi2020trilingual,
  title={Trilingual Semantic Embeddings of Visually Grounded Speech with Self-Attention Mechanisms},
  author={Ohishi, Yasunori and Kimura, Akisato and Kawanishi, Takahito and Kashino, Kunio and Harwath, David and Glass, James},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4352--4356},
  year={2020},
  organization={IEEE}
}


@article{roy2005towards,
  title={Towards situated speech understanding: Visual context priming of language models},
  author={Roy, Deb and Mukherjee, Niloy},
  journal={Computer Speech \& Language},
  volume={19},
  number={2},
  pages={227--248},
  year={2005},
  publisher={Elsevier}
}


@inproceedings{roy2002trainable,
  title={A trainable spoken language understanding system for visual object selection},
  author={Roy, Deb and Gorniak, Peter and Mukherjee, Niloy and Juster, Josh},
  booktitle={Seventh International Conference on Spoken Language Processing},
  year={2002}
}


@article{merkx2019learning,
  title={Learning semantic sentence representations from visually grounded language without lexical knowledge},
  author={Merkx, Danny and Frank, Stefan L},
  journal={Natural Language Engineering},
  volume={25},
  number={4},
  pages={451--466},
  year={2019},
  publisher={Cambridge University Press}
}


@inproceedings{dindo2010probabilistic,
  title={A probabilistic approach to learning a visually grounded language model through human-robot interaction},
  author={Dindo, Haris and Zambuto, Daniele},
  booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={790--796},
  year={2010},
  organization={IEEE}
}

@inproceedings{kamper2018visually,
  title={Visually Grounded Cross-Lingual Keyword Spotting in Speech},
  author={Kamper, Herman and Roth, Michael},
  booktitle={Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages},
  pages={253--257},
  year={2018}
}


@article{roy2000learning,
  title={Learning visually grounded words and syntax of natural spoken language},
  author={Roy, Deb},
  journal={Evolution of communication},
  volume={4},
  number={1},
  pages={33--56},
  year={2000},
  publisher={John Benjamins}
}


@article{roy2002learning,
  title={Learning visually grounded words and syntax for a scene description task},
  author={Roy, Deb K},
  journal={Computer speech \& language},
  volume={16},
  number={3-4},
  pages={353--385},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{marelli2014sick,
  title={A SICK cure for the evaluation of compositional distributional semantic models},
  author={Marelli, Marco and Menini, Stefano and Baroni, Marco and Bentivogli, Luisa and Bernardi, Raffaella and Zamparelli, Roberto},
  booktitle={Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
  pages={216--223},
  year={2014}
}


@inproceedings{havard2020catplayinginthesnow,
    title = "Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech",
    author = "Havard, William  and
      Besacier, Laurent  and
      Chevrot, Jean-Pierre",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.22",
    doi = "10.18653/v1/2020.conll-1.22",
    pages = "291--301",
}


@misc{havard2017speech,
  title={{SPEECH-COCO}: 600k visually grounded spoken captions aligned to {MSCOCO} data set},
  author={Havard, William and Besacier, Laurent and Rosec, Olivier},
  eprint={1707.08435},
  year={2017},
  note={Preprint: \url{https://arxiv.org/abs/1707.08435}}
}


@inproceedings{scharenborg2018linguistic,
  title={Linguistic unit discovery from multi-modal inputs in unwritten languages: Summary of the “Speaking rosetta” JSALT 2017 workshop},
  author={Scharenborg, Odette and Besacier, Laurent and Black, Alan and Hasegawa-Johnson, Mark and Metze, Florian and Neubig, Graham and St{\"u}ker, Sebastian and Godard, Pierre and M{\"u}ller, Markus and Ondel, Lucas and others},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4979--4983},
  year={2018},
  organization={IEEE}
}


@inproceedings{havard2019models,
  title={Models of visually grounded speech signal pay attention to nouns: a bilingual experiment on {E}nglish and {J}apanese},
  author={Havard, William N and Chevrot, Jean-Pierre and Besacier, Laurent},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8618--8622},
  year={2019},
  organization={IEEE}
}

@inproceedings{havard2019word,
    title = "Word Recognition, Competition, and Activation in a Model of Visually Grounded Speech",
    author = "Havard, William N.  and
      Chevrot, Jean-Pierre  and
      Besacier, Laurent",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1032",
    doi = "10.18653/v1/K19-1032",
    pages = "339--348",
}


@misc{scholten2020learning,
  title={Learning to Recognise Words using Visually Grounded Speech},
  author={Scholten, Sebastiaan and Merkx, Danny and Scharenborg, Odette},
  eprint={arXiv preprint arXiv:2006.00512},
  note={Preprint: \url{https://arxiv.org/abs/2006.00512}},
  year={2020}
}


@misc{wang2020show,
  title={Show and Speak: Directly Synthesize Spoken Description of Images},
  author={Wang, Xinsheng and Feng, Siyuan and Zhu, Jihua and Hasegawa-Johnson, Mark and Scharenborg, Odette},
  eprint={2010.12267},
  year={2020}
}



@inproceedings{kamper2019semantic,
  title={Semantic query-by-example speech search using visual grounding},
  author={Kamper, Herman and Anastassiou, Aristotelis and Livescu, Karen},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7120--7124},
  year={2019},
  organization={IEEE}
}



@inproceedings{eloff2019multimodal,
  title={Multimodal one-shot learning of speech and images},
  author={Eloff, Ryan and Engelbrecht, Herman A and Kamper, Herman},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8623--8627},
  year={2019},
  organization={IEEE}
}


@article{kamper2019semantic-ieee,
  title={Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech},
  author={Kamper, Herman and Shakhnarovich, Gregory and Livescu, Karen},
  journal={IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
  volume={27},
  number={1},
  pages={89--98},
  year={2019},
  publisher={IEEE Press}
}



@inproceedings{kamper2017visually,
  title={Visually Grounded Learning of Keyword Prediction from Untranscribed Speech},
  author={Kamper, Herman and Settle, Shane and Shakhnarovich, Gregory and Livescu, Karen},
  booktitle={Interspeech},
  pages={3677--3681},
  year={2017}
}


@inproceedings{nortje2020unsupervised,
  title={Unsupervised vs. Transfer Learning for Multimodal One-Shot Matching of Speech and Images},
  author={Nortje, Leanne and Kamper, Herman},
  booktitle={Interspeech},
  pages={2712--2716},
  year={2020}
}



@inproceedings{leidal2017learning,
  title={Learning modality-invariant representations for speech and images},
  author={Leidal, Kenneth and Harwath, David and Glass, James},
  booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={424--429},
  year={2017},
  organization={IEEE}
}



@phdthesis{harwath2018learning,
  title={Learning spoken language through vision},
  author={Harwath, David Frank},
  year={2018},
  school={Massachusetts Institute of Technology}
}


@inproceedings{harwath2018vision,
  title={Vision as an interlingua: Learning multilingual semantic embeddings of untranscribed speech},
  author={Harwath, David and Chuang, Galen and Glass, James},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4969--4973},
  year={2018},
  organization={IEEE}
}


@inproceedings{azuh2019towards,
  title={Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio},
  author={Azuh, Emmanuel and Harwath, David and Glass, James},
  year={2019},
  booktitle={Interspeech}
}


@inproceedings{harwath2019towards,
  title={Towards visually grounded sub-word speech unit discovery},
  author={Harwath, David and Glass, James},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3017--3021},
  year={2019},
  organization={IEEE}
}


@inproceedings{harwath2016unsupervised,
  title={Unsupervised learning of spoken language with visual context},
  author={Harwath, David and Torralba, Antonio and Glass, James},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1858--1866},
  year={2016}
}


@article{cleveland1979robust,
  title={Robust locally weighted regression and smoothing scatterplots},
  author={Cleveland, William S},
  journal={Journal of the American statistical association},
  volume={74},
  number={368},
  pages={829--836},
  year={1979},
  publisher={Taylor \& Francis}
}

@inproceedings{davis2020discourse,
    title = "Discourse structure interacts with reference but not syntax in neural language models",
    author = "Davis, Forrest  and
      van Schijndel, Marten",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.32",
    doi = "10.18653/v1/2020.conll-1.32",
    pages = "396--407",
}

@article{fereidooni2020understanding,
  title={Understanding and Improving Word Embeddings through a Neuroscientific Lens},
  author={Fereidooni, Sam and Mocz, Viola and Radev, Dragomir and Chun, Marvin},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}


@inproceedings{harwath2015deep,
  title={Deep multimodal semantic embeddings for speech and images},
  author={Harwath, David and Glass, James},
  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  pages={237--244},
  year={2015},
  organization={IEEE}
}


@INPROCEEDINGS{
         Povey_ASRU2011,
         author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
       keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
          month = dec,
          title = {The Kaldi Speech Recognition Toolkit},
      booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
           year = {2011},
      publisher = {IEEE Signal Processing Society},
       location = {Hilton Waikoloa Village, Big Island, Hawaii, US},
           note = {IEEE Catalog No.: CFP11SRW-USB},
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{harwath2018jointly,
  title={Jointly discovering visual objects and spoken words from raw sensory input},
  author={Harwath, David and Recasens, Adria and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={649--665},
  year={2018}
}


@article{harwath2020jointly,
  title={Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input},
  author={Harwath, David and Recasens, Adri{\`a} and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
  journal={International Journal of Computer Vision},
  volume={128},
  number={3},
  pages={620--641},
  year={2020},
  publisher={Springer}
}


@inproceedings{Merkx2019,
  author={Danny Merkx and Stefan L. Frank and Mirjam Ernestus},
  title={{Language Learning Using Speech to Image Retrieval}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1841--1845},
  doi={10.21437/Interspeech.2019-3067},
  url={http://dx.doi.org/10.21437/Interspeech.2019-3067}
}


@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}



@inproceedings{vanoord2017neural,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and  Koray Kavukcuoglu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6306--6315},
  year={2017}
}


@phdthesis{schatz2016abx,
  title={ABX-discriminability measures and applications},
  author={Schatz, Thomas},
  school={Université Paris 6 (UPMC)},
  year={2016}
}


@inproceedings{adi2016fine,
  author    = {Yossi Adi and
               Einat Kermany and
               Yonatan Belinkov and
               Ofer Lavi and
               Yoav Goldberg},
  title     = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction
               Tasks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=BJh6Ztuxl},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/AdiKBLG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kriegeskorte2008representational,
  title={Representational similarity analysis-connecting the branches of systems neuroscience},
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A},
  journal={Frontiers in systems neuroscience},
  volume={2},
  pages={4},
  year={2008},
  publisher={Frontiers}
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}

@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@inproceedings{voita2020informationtheoretic,
    title = "Information-Theoretic Probing with Minimum Description Length",
    author = "Voita, Elena  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.14",
    doi = "10.18653/v1/2020.emnlp-main.14",
    pages = "183--196",
}

@inproceedings{rosenberg_v-measure_2007,
    title = "{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
    author = "Rosenberg, Andrew  and
      Hirschberg, Julia",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D07-1043",
    pages = "410--420",
}


@inproceedings{haghani_audio_2018,
	title = {From {Audio} to {Semantics}: {Approaches} to {End}-to-{End} {Spoken} {Language} {Understanding}},
	shorttitle = {From {Audio} to {Semantics}},
	doi = {10.1109/SLT.2018.8639043},
	booktitle = {2018 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Haghani, Parisa and Narayanan, Arun and Bacchiani, Michiel and Chuang, Galen and Gaur, Neeraj and Moreno, Pedro and Prabhavalkar, Rohit and Qu, Zhongdi and Waters, Austin},
	month = dec,
	year = {2018},
	note = {ISSN: null},
	pages = {720--726},
	file = {IEEE Xplore Abstract Record:/home/bjrhigy/Zotero/storage/CKET9KPE/8639043.html:text/html;IEEE Xplore Full Text PDF:/home/bjrhigy/Zotero/storage/YWPP4D7R/Haghani et al. - 2018 - From Audio to Semantics Approaches to End-to-End .pdf:application/pdf}
}


@article{dunbar2019zero,
  title={The zero resource speech challenge 2019: TTS without T},
  author={Dunbar, Ewan and Algayres, Robin and Karadayi, Julien and Bernard, Mathieu and Benjumea, Juan and Cao, Xuan-Nga and Miskic, Lucie and Dugrain, Charlotte and Ondel, Lucas and Black, Alan W and others},
  journal={arXiv preprint arXiv:1904.11469},
  year={2019}
}

@inproceedings{higy2020textual,
    title = "Textual {S}upervision for {V}isually {G}rounded {S}poken {L}anguage {U}nderstanding",
    author = "Higy, Bertrand  and
      Elliott, Desmond  and
      Chrupa{\l}a, Grzegorz",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.244",
    doi = "10.18653/v1/2020.findings-emnlp.244",
    pages = "2698--2709",
}


@misc{van2020vector,
  title={Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge},
  author={van Niekerk, Benjamin and Nortje, Leanne and Kamper, Herman},
  note={Preprint: \url{https://arxiv.org/abs/2005.09409}},
  year={2020}
}


@article{chung2020vector,
  title={Vector-Quantized Autoregressive Predictive Coding},
  author={Chung, Yu-An and Tang, Hao and Glass, James},
  journal={arXiv preprint arXiv:2005.08392},
  year={2020}
}


@inproceedings{harwath2020learning,
  author    = {David Harwath and
               Wei{-}Ning Hsu and
               James R. Glass},
  title     = {Learning Hierarchical Discrete Linguistic Units from Visually-Grounded
               Speech},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=B1elCp4KwH},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HarwathHG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{chorowski_unsupervised_2019,
	title = {Unsupervised {Speech} {Representation} {Learning} {Using} {WaveNet} {Autoencoders}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2938863},
	abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, A{\"a}ron},
	month = dec,
	year = {2019},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {learning (artificial intelligence), neural nets, Neural networks, unsupervised learning, Feature extraction, Gaussian processes, speech processing, Decoding, Task analysis, vector quantisation, acoustic signal processing, Speech processing, acoustic unit discovery, Autoencoder, autoencoder models, autoencoding neural networks, discrete vector quantized VAE, Gaussian variational autoencoder, high capacity WaveNet decoder, high level semantic content, latent representation, latent representations, low level details, phoneme identities, phonetic content, Phonetics, pitch contour, Prototypes, simple dimensionality reduction bottleneck, speech representation learning, speech waveforms, unsupervised extraction, unsupervised speech representation, VQ-VAE, WaveNet autoencoders, unsupervised unit discovery, zerospeech 2017},
	pages = {2041--2053},
	file = {IEEE Xplore Abstract Record:/home/bjrhigy/Zotero/storage/IPQ2QKKY/8822475.html:text/html;IEEE Xplore Full Text PDF:/home/bjrhigy/Zotero/storage/ZISUFIFF/Chorowski et al. - 2019 - Unsupervised Speech Representation Learning Using .pdf:application/pdf}
}


@article{eloff_unsupervised_2019,
	title = {Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks},
	url = {http://arxiv.org/abs/1904.07556},
	abstract = {For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.},
	urldate = {2020-06-24},
	journal = {arXiv:1904.07556 [cs, eess]},
	author = {Eloff, Ryan and Nortje, Andr{\'e} and van Niekerk, Benjamin and Govender, Avashna and Nortje, Leanne and Pretorius, Arnu and van Biljon, Elan and van der Westhuizen, Ewald and van Staden, Lisa and Kamper, Herman},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.07556},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2019},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/68I73Y4W/Eloff et al. - 2019 - Unsupervised acoustic unit discovery for speech sy.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/SGTC28RP/1904.html:text/html}
}


@article{tjandra_vqvae_2019,
	title = {{VQVAE} {Unsupervised} {Unit} {Discovery} and {Multi}-scale {Code2Spec} {Inverter} for {Zerospeech} {Challenge} 2019},
	url = {http://arxiv.org/abs/1905.11449},
	abstract = {We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.},
	urldate = {2020-06-24},
	journal = {arXiv:1905.11449 [cs, eess]},
	author = {Tjandra, Andros and Sisman, Berrak and Zhang, Mingyang and Sakti, Sakriani and Li, Haizhou and Nakamura, Satoshi},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11449},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2019},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/VS6MSHZG/Tjandra et al. - 2019 - VQVAE Unsupervised Unit Discovery and Multi-scale .pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/QHPS2B9V/1905.html:text/html}
}


@article{tjandra_transformer_2020,
	title = {Transformer {VQ}-{VAE} for {Unsupervised} {Unit} {Discovery} and {Speech} {Synthesis}: {ZeroSpeech} 2020 {Challenge}},
	shorttitle = {Transformer {VQ}-{VAE} for {Unsupervised} {Unit} {Discovery} and {Speech} {Synthesis}},
	url = {http://arxiv.org/abs/2005.11676},
	abstract = {In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019. The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels. In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) re-synthesize the audio from novel speakers. The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate. Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformer-based inverter for the speech synthesis given the extracted codebook. Additionally, we also explored several regularization methods to improve performance even further.},
	urldate = {2020-06-24},
	journal = {arXiv:2005.11676 [cs, eess]},
	author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
	month = may,
	year = {2020},
	note = {arXiv: 2005.11676},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, VQ-VAE, zerospeech 2020},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/VDQIHUZ6/Tjandra et al. - 2020 - Transformer VQ-VAE for Unsupervised Unit Discovery.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/BN4F4W78/2005.html:text/html}
}

